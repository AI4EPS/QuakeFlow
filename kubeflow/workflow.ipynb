{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Detection Workflow using QuakeFlow\n",
    "\n",
    "## Outline\n",
    "\n",
    "Here we show an example of the current modules in QuakeFlow\n",
    "\n",
    "1. Download data using Obpsy:\n",
    "\n",
    "    [FDSN web service client for ObsPy](https://docs.obspy.org/packages/obspy.clients.fdsn.html#module-obspy.clients.fdsn)\n",
    "    \n",
    "    [Mass Downloader for FDSN Compliant Web Services](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.mass_downloader.html#module-obspy.clients.fdsn.mass_downloader)\n",
    "\n",
    "2. PhaseNet for picking P/S phases\n",
    "\n",
    "    Find more details in [PhaseNet github page](https://wayneweiqiang.github.io/PhaseNet/)\n",
    "\n",
    "3. GaMMA for associating picking and estimate approximate location and magnitude\n",
    "\n",
    "    Find more details in [GaMMA github page](https://wayneweiqiang.github.io/GaMMA/)\n",
    "    \n",
    "4. Earthquake location, magnitude estimation, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install [miniconda](https://docs.conda.io/en/latest/miniconda.html) and download packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T07:24:29.845680Z",
     "iopub.status.busy": "2021-07-23T07:24:29.845272Z",
     "iopub.status.idle": "2021-07-23T07:24:29.922435Z",
     "shell.execute_reply": "2021-07-23T07:24:29.917867Z",
     "shell.execute_reply.started": "2021-07-23T07:24:29.845649Z"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "git clone https://github.com/wayneweiqiang/PhaseNet.git\n",
    "git clone https://github.com/wayneweiqiang/GaMMA.git\n",
    "conda env update -f=env.yml -n base\n",
    "```\n",
    "\n",
    "**Second option: install to quakeflow environment, but need to select jupyter notebook kernel to quakeflow**\n",
    "```\n",
    "conda env create -f=env.yml -n quakeflow\n",
    "python -m ipykernel install --user --name=quakeflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:42.837909Z",
     "iopub.status.busy": "2022-08-11T15:42:42.837825Z",
     "iopub.status.idle": "2022-08-11T15:42:43.073297Z",
     "shell.execute_reply": "2022-08-11T15:42:43.073016Z",
     "shell.execute_reply.started": "2022-08-11T15:42:42.837882Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:24.188176Z",
     "iopub.status.busy": "2022-08-11T20:03:24.187935Z",
     "iopub.status.idle": "2022-08-11T20:03:24.196474Z",
     "shell.execute_reply": "2022-08-11T20:03:24.196164Z",
     "shell.execute_reply.started": "2022-08-11T20:03:24.188158Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.use(\"agg\")\n",
    "%matplotlib inline\n",
    "\n",
    "# region_name = \"Demo\"\n",
    "# region_name = \"Ridgecrest\"\n",
    "# region_name = \"SaltonSea\"\n",
    "# region_name = \"Ridgecrest\"\n",
    "# region_name = \"SanSimeon\"\n",
    "# region_name = \"Italy\"\n",
    "# region_name = \"PNSN\"\n",
    "# region_name = \"Hawaii3\"\n",
    "region_name = \"Hawaii4\"\n",
    "# region_name = \"Hawaii_Loa\"\n",
    "# region_name = \"SierraNegra\"\n",
    "# region_name = \"PuertoRico\"\n",
    "# region_name = \"SmithValley\"\n",
    "# region_name = \"Antilles\"\n",
    "# region_name = \"LongValley\"\n",
    "\n",
    "dir_name = region_name\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "root_dir = lambda x: os.path.join(dir_name, x)\n",
    "\n",
    "run_local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:24.601595Z",
     "iopub.status.busy": "2022-08-11T20:03:24.601484Z",
     "iopub.status.idle": "2022-08-11T20:03:24.613499Z",
     "shell.execute_reply": "2022-08-11T20:03:24.613194Z",
     "shell.execute_reply.started": "2022-08-11T20:03:24.601583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_config(\n",
    "    region_name,\n",
    "    index_json: OutputPath(\"json\"),\n",
    "    config_json: OutputPath(\"json\"),\n",
    "    datetime_json: OutputPath(\"json\"),\n",
    "    num_parallel: int = 1,\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> list:\n",
    "\n",
    "    import datetime\n",
    "    import json\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import numpy as np\n",
    "    import obspy\n",
    "\n",
    "    degree2km = np.pi * 6371 / 180\n",
    "\n",
    "    if region_name == \"Demo\":\n",
    "        center = (-117.504, 35.705)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2019-07-04T17\")\n",
    "        endtime = obspy.UTCDateTime(\"2019-07-04T19\")\n",
    "        client = \"SCEDC\"\n",
    "        network_list = [\"CI\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Ridgecrest\":\n",
    "        center = (-117.504, 35.705)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2019-07-04T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2019-07-10T00\")\n",
    "        client = \"SCEDC\"\n",
    "        network_list = [\"CI\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Hawaii\":\n",
    "        center = (-155.32, 19.39)\n",
    "        horizontal_degree = 2.0\n",
    "        vertical_degree = 2.0\n",
    "        starttime = obspy.UTCDateTime(\"2018-01-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-08-12T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"HV\", \"PT\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Hawaii2\":\n",
    "        center = (-155.32, 19.39)\n",
    "        horizontal_degree = 2.0\n",
    "        vertical_degree = 2.0\n",
    "        starttime = obspy.UTCDateTime(\"2022-08-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-08-16T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"HV\", \"PT\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Hawaii3\":\n",
    "        center = (-155.32, 19.39)\n",
    "        horizontal_degree = 2.0\n",
    "        vertical_degree = 2.0\n",
    "        # starttime = obspy.UTCDateTime(\"2010-01-01T00\")\n",
    "        starttime = obspy.UTCDateTime(\"2022-08-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-11-09T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"HV\"]\n",
    "        channel_list = \"HH*,BH*,EH*\"\n",
    "\n",
    "    if region_name == \"Hawaii4\":\n",
    "        center = (-155.32, 19.39)\n",
    "        horizontal_degree = 2.0\n",
    "        vertical_degree = 2.0\n",
    "        starttime = obspy.UTCDateTime(\"2010-01-01T00\")\n",
    "        # starttime = obspy.UTCDateTime(\"2022-08-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-11-09T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"HV\"]\n",
    "        channel_list = \"HH*,BH*,EH*\"\n",
    "\n",
    "    if region_name == \"Hawaii_Loa\":\n",
    "        center = (-155.602737, 19.451827)\n",
    "        horizontal_degree = 0.3\n",
    "        vertical_degree = 0.3\n",
    "        starttime = obspy.UTCDateTime(\"2010-01-01T00\")\n",
    "        # starttime = obspy.UTCDateTime(\"2022-08-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-11-09T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"HV\"]\n",
    "        channel_list = \"HH*,BH*,EH*\"\n",
    "\n",
    "    if region_name == \"PuertoRico\":\n",
    "        center = (-66.5, 18)\n",
    "        horizontal_degree = 3.0\n",
    "        vertical_degree = 2.0\n",
    "        starttime = obspy.UTCDateTime(\"2018-05-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2021-11-01T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,BH*,HN*\"\n",
    "\n",
    "    if region_name == \"SaltonSea\":\n",
    "        center = (-115.53, 32.98)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2020-10-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2020-10-01T02\")\n",
    "        client = \"SCEDC\"\n",
    "        network_list = [\"CI\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"2003SanSimeon\":\n",
    "        center = (-121.101, 35.701)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2003-12-22T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2003-12-24T00\")\n",
    "        client = \"NCEDC\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Italy\":\n",
    "        center = (13.188, 42.723)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2016-08-24T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2016-08-26T00\")\n",
    "        client = \"INGV\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"SmithValley\":\n",
    "        center = (-119.5, 38.51)\n",
    "        horizontal_degree = 1.0\n",
    "        vertical_degree = 1.0\n",
    "        starttime = obspy.UTCDateTime(\"2021-07-08T00:00\")\n",
    "        endtime = obspy.UTCDateTime(\"2021-07-16T00:00\")\n",
    "        client = \"NCEDC\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"Antilles\":\n",
    "        center = (-61.14867, 14.79683)\n",
    "        horizontal_degree = 0.2\n",
    "        vertical_degree = 0.2\n",
    "        starttime = obspy.UTCDateTime(\"2021-04-10T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2021-04-15T00\")\n",
    "        client = \"RESIF\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "    if region_name == \"LongValley\":\n",
    "        # center = (-118.8, 37.6+0.3)\n",
    "        # horizontal_degree = 3.0\n",
    "        # vertical_degree = 3.0\n",
    "        center = (-118.8 - 0.1, 37.6)\n",
    "        horizontal_degree = 1.5\n",
    "        vertical_degree = 1.5\n",
    "        starttime = obspy.UTCDateTime(\"2020-01-01T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2022-08-11T00\")\n",
    "        client = \"NCEDC\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"HH*,EH*\"\n",
    "\n",
    "    if region_name == \"SierraNegra\":\n",
    "        center = (-91.13, -0.81)\n",
    "        horizontal_degree = 1.5\n",
    "        vertical_degree = 1.5\n",
    "        starttime = obspy.UTCDateTime(\"2009-07-22T00\")\n",
    "        endtime = obspy.UTCDateTime(\"2011-06-20T00\")\n",
    "        client = \"IRIS\"\n",
    "        network_list = [\"*\"]\n",
    "        channel_list = \"BH*,HH*,EH*\"\n",
    "\n",
    "    ####### save config ########\n",
    "    config = {}\n",
    "    config[\"region\"] = region_name\n",
    "    config[\"center\"] = center\n",
    "    config[\"xlim_degree\"] = [\n",
    "        center[0] - horizontal_degree / 2,\n",
    "        center[0] + horizontal_degree / 2,\n",
    "    ]\n",
    "    config[\"ylim_degree\"] = [\n",
    "        center[1] - vertical_degree / 2,\n",
    "        center[1] + vertical_degree / 2,\n",
    "    ]\n",
    "    config[\"min_longitude\"] = center[0] - horizontal_degree / 2\n",
    "    config[\"max_longitude\"] = center[0] + horizontal_degree / 2\n",
    "    config[\"min_latitude\"] = center[1] - vertical_degree / 2\n",
    "    config[\"max_latitude\"] = center[1] + vertical_degree / 2\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime.datetime.isoformat(timespec=\"milliseconds\")\n",
    "    config[\"endtime\"] = endtime.datetime.isoformat(timespec=\"milliseconds\")\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"client\"] = client\n",
    "\n",
    "    ## PhaseNet\n",
    "    config[\"phasenet\"] = {}\n",
    "    ## GaMMA\n",
    "    config[\"gamma\"] = {}\n",
    "    ## HypoDD\n",
    "    config[\"hypodd\"] = {\"MAXEVENT\": 1e4}\n",
    "\n",
    "    with open(config_json, \"w\") as fp:\n",
    "        json.dump(config, fp, indent=2)\n",
    "\n",
    "    print(json.dumps(config, indent=4))\n",
    "\n",
    "    ####### set paraell for cloud ########\n",
    "    ## split data by hours\n",
    "    # one_day = datetime.timedelta(days=1)\n",
    "    one_hour = datetime.timedelta(hours=1)\n",
    "    starttimes = []\n",
    "    tmp_start = starttime\n",
    "    while tmp_start < endtime:\n",
    "        starttimes.append(tmp_start.datetime.isoformat(timespec=\"milliseconds\"))\n",
    "        tmp_start += one_hour\n",
    "    with open(datetime_json, \"w\") as fp:\n",
    "        json.dump(\n",
    "            {\"starttimes\": starttimes, \"interval\": one_hour.total_seconds()},\n",
    "            fp,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    ## split stattimes into N parallel process\n",
    "    if num_parallel == 0:\n",
    "        num_parallel = min(60, int((len(starttimes) - 1) // 6 + 1))\n",
    "        # num_parallel = min(30, int((len(starttimes)-1)//6+1))\n",
    "        # num_parallel = min(24, len(starttimes))\n",
    "    idx = [x.tolist() for x in np.array_split(np.arange(len(starttimes)), num_parallel)]\n",
    "\n",
    "    with open(index_json, \"w\") as fp:\n",
    "        json.dump(idx, fp, indent=2)\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/config.json\",\n",
    "            config_json,\n",
    "        )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/datetime.json\",\n",
    "            datetime_json,\n",
    "        )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/index.json\",\n",
    "            index_json,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:25.108832Z",
     "iopub.status.busy": "2022-08-11T20:03:25.108714Z",
     "iopub.status.idle": "2022-08-11T20:03:31.275226Z",
     "shell.execute_reply": "2022-08-11T20:03:31.274848Z",
     "shell.execute_reply.started": "2022-08-11T20:03:25.108820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    idx = set_config(\n",
    "        region_name,\n",
    "        root_dir(\"index.json\"),\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"datetimes.json\"),\n",
    "        num_parallel=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:31.275970Z",
     "iopub.status.busy": "2022-08-11T20:03:31.275892Z",
     "iopub.status.idle": "2022-08-11T20:03:31.308479Z",
     "shell.execute_reply": "2022-08-11T20:03:31.308169Z",
     "shell.execute_reply.started": "2022-08-11T20:03:31.275959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_op = comp.func_to_container_op(\n",
    "    set_config,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"numpy\", \"obspy\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download events of the standard catalog\n",
    "\n",
    "This catalog is not used by QuakeFolow. It is only used for comparing detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:31.309035Z",
     "iopub.status.busy": "2022-08-11T20:03:31.308926Z",
     "iopub.status.idle": "2022-08-11T20:03:31.314738Z",
     "shell.execute_reply": "2022-08-11T20:03:31.314519Z",
     "shell.execute_reply.started": "2022-08-11T20:03:31.309023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_events(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    standard_catalog: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    import pickle\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import obspy\n",
    "    import pandas as pd\n",
    "    from obspy.clients.fdsn import Client\n",
    "\n",
    "    # matplotlib.use(\"agg\")\n",
    "    # %matplotlib inline\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    ####### IRIS catalog ########\n",
    "    try:\n",
    "        events = Client(config[\"client\"]).get_events(\n",
    "            starttime=config[\"starttime\"],\n",
    "            endtime=config[\"endtime\"],\n",
    "            minlongitude=config[\"xlim_degree\"][0],\n",
    "            maxlongitude=config[\"xlim_degree\"][1],\n",
    "            minlatitude=config[\"ylim_degree\"][0],\n",
    "            maxlatitude=config[\"ylim_degree\"][1],\n",
    "            # filename='events.xml',\n",
    "        )\n",
    "    except:\n",
    "        events = Client(\"iris\").get_events(\n",
    "            starttime=config[\"starttime\"],\n",
    "            endtime=config[\"endtime\"],\n",
    "            minlongitude=config[\"xlim_degree\"][0],\n",
    "            maxlongitude=config[\"xlim_degree\"][1],\n",
    "            minlatitude=config[\"ylim_degree\"][0],\n",
    "            maxlatitude=config[\"ylim_degree\"][1],\n",
    "            # filename='events.xml',\n",
    "        )\n",
    "\n",
    "    #     events = obspy.read_events('events.xml')\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "    #     events.plot('local', outfile=\"events.png\")\n",
    "    #     events.plot('local')\n",
    "\n",
    "    ####### Save catalog ########\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        if len(event.magnitudes) > 0:\n",
    "            catalog[\"time\"].append(event.origins[0].time.datetime)\n",
    "            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n",
    "            catalog[\"longitude\"].append(event.origins[0].longitude)\n",
    "            catalog[\"latitude\"].append(event.origins[0].latitude)\n",
    "            catalog[\"depth(m)\"].append(event.origins[0].depth)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "    catalog.to_csv(\n",
    "        standard_catalog,\n",
    "        # sep=\"\\t\",\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "        date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\"],\n",
    "    )\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/standard_catalog.csv\",\n",
    "            standard_catalog,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    ####### Plot catalog ########\n",
    "    plt.figure()\n",
    "    plt.plot(catalog[\"longitude\"], catalog[\"latitude\"], \".\", markersize=1)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "    # plt.savefig(os.path.join(data_path, \"events_loc.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot_date(catalog[\"time\"], catalog[\"magnitude\"], \".\", markersize=1)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(f\"Number of events: {len(events)}\")\n",
    "    # plt.savefig(os.path.join(data_path, \"events_mag_time.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:03:31.315388Z",
     "iopub.status.busy": "2022-08-11T20:03:31.315288Z",
     "iopub.status.idle": "2022-08-11T20:04:10.274910Z",
     "shell.execute_reply": "2022-08-11T20:04:10.274498Z",
     "shell.execute_reply.started": "2022-08-11T20:03:31.315377Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_events(root_dir(\"config.json\"), root_dir(\"standard_catalog.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:04:10.275411Z",
     "iopub.status.busy": "2022-08-11T20:04:10.275332Z",
     "iopub.status.idle": "2022-08-11T20:04:10.287343Z",
     "shell.execute_reply": "2022-08-11T20:04:10.287063Z",
     "shell.execute_reply.started": "2022-08-11T20:04:10.275399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_events_op = comp.func_to_container_op(\n",
    "    download_events,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"obspy\",\n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "        \"minio\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:04:10.288008Z",
     "iopub.status.busy": "2022-08-11T20:04:10.287927Z",
     "iopub.status.idle": "2022-08-11T20:04:10.294533Z",
     "shell.execute_reply": "2022-08-11T20:04:10.294282Z",
     "shell.execute_reply.started": "2022-08-11T20:04:10.287997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_stations(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    station_json: OutputPath(\"json\"),\n",
    "    station_pkl: OutputPath(\"pickle\"),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    import pickle\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import obspy\n",
    "    import pandas as pd\n",
    "    from obspy.clients.fdsn import Client\n",
    "\n",
    "    # matplotlib.use(\"agg\")\n",
    "    # %matplotlib inline\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    print(\"Network:\", \",\".join(config[\"networks\"]))\n",
    "\n",
    "    ####### Download stations ########\n",
    "    stations = Client(config[\"client\"]).get_stations(\n",
    "        network=\",\".join(config[\"networks\"]),\n",
    "        station=\"*\",\n",
    "        starttime=config[\"starttime\"],\n",
    "        endtime=config[\"endtime\"],\n",
    "        minlongitude=config[\"xlim_degree\"][0],\n",
    "        maxlongitude=config[\"xlim_degree\"][1],\n",
    "        minlatitude=config[\"ylim_degree\"][0],\n",
    "        maxlatitude=config[\"ylim_degree\"][1],\n",
    "        channel=config[\"channels\"],\n",
    "        level=\"response\",\n",
    "        # filename=\"stations.xml\"\n",
    "    )\n",
    "\n",
    "    #     stations = obspy.read_inventory(\"stations.xml\")\n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "    # stations.plot('local', outfile=\"stations.png\")\n",
    "    #     stations.plot('local')\n",
    "\n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    if chn.code[-1] not in station_locs[sid][\"component\"]:\n",
    "                        station_locs[sid][\"component\"].append(chn.code[-1])\n",
    "                        station_locs[sid][\"response\"].append(round(chn.response.instrument_sensitivity.value, 2))\n",
    "                else:\n",
    "                    tmp_dict = {\n",
    "                        \"longitude\": chn.longitude,\n",
    "                        \"latitude\": chn.latitude,\n",
    "                        \"elevation(m)\": chn.elevation,\n",
    "                        \"component\": [\n",
    "                            chn.code[-1],\n",
    "                        ],\n",
    "                        \"response\": [\n",
    "                            round(chn.response.instrument_sensitivity.value, 2),\n",
    "                        ],\n",
    "                        \"unit\": chn.response.instrument_sensitivity.input_units.lower(),\n",
    "                    }\n",
    "                    station_locs[sid] = tmp_dict\n",
    "\n",
    "    with open(station_json, \"w\") as fp:\n",
    "        json.dump(station_locs, fp, indent=2)\n",
    "\n",
    "    with open(station_pkl, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/stations.json\",\n",
    "            station_json,\n",
    "        )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/stations.pkl\",\n",
    "            station_pkl,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    ######## Plot stations ########\n",
    "    station_locs = pd.DataFrame.from_dict(station_locs, orient=\"index\")\n",
    "    plt.figure()\n",
    "    plt.plot(station_locs[\"longitude\"], station_locs[\"latitude\"], \"^\", label=\"Stations\")\n",
    "    plt.xlabel(\"X (km)\")\n",
    "    plt.ylabel(\"Y (km)\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "    #     plt.savefig(os.path.join(data_path, \"stations_loc.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:04:10.295020Z",
     "iopub.status.busy": "2022-08-11T20:04:10.294907Z",
     "iopub.status.idle": "2022-08-11T20:04:15.789908Z",
     "shell.execute_reply": "2022-08-11T20:04:15.789661Z",
     "shell.execute_reply.started": "2022-08-11T20:04:10.295009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_stations(root_dir(\"config.json\"), root_dir(\"stations.json\"), root_dir(\"stations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T20:04:15.790410Z",
     "iopub.status.busy": "2022-08-11T20:04:15.790300Z",
     "iopub.status.idle": "2022-08-11T20:04:15.801936Z",
     "shell.execute_reply": "2022-08-11T20:04:15.801582Z",
     "shell.execute_reply.started": "2022-08-11T20:04:15.790399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_stations_op = comp.func_to_container_op(\n",
    "    download_stations,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"obspy\",\n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "        \"minio\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download waveform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.171415Z",
     "iopub.status.busy": "2022-08-11T15:42:43.171308Z",
     "iopub.status.idle": "2022-08-11T15:42:43.180260Z",
     "shell.execute_reply": "2022-08-11T15:42:43.179978Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.171400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_waveform(\n",
    "    node_i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    datetime_json: InputPath(\"json\"),\n",
    "    station_pkl: InputPath(\"pickle\"),\n",
    "    fname_csv: OutputPath(str),\n",
    "    data_path: str,\n",
    "    bucket_name: str = \"waveforms\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> str:\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    import pickle\n",
    "    import random\n",
    "    import threading\n",
    "    import time\n",
    "\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    upload_minio = False\n",
    "    # try:\n",
    "    #     from minio import Minio\n",
    "\n",
    "    #     minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "    #     if not minioClient.bucket_exists(bucket_name):\n",
    "    #         minioClient.make_bucket(bucket_name)\n",
    "    #     upload_minio = True\n",
    "    # except Exception as err:\n",
    "    #     # print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "    #     pass\n",
    "\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[node_i]\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    with open(datetime_json, \"r\") as fp:\n",
    "        tmp = json.load(fp)\n",
    "        starttimes = tmp[\"starttimes\"]\n",
    "        interval = tmp[\"interval\"]\n",
    "    with open(station_pkl, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "\n",
    "    waveform_dir = os.path.join(data_path, config[\"region\"], \"waveforms\")\n",
    "    if not os.path.exists(waveform_dir):\n",
    "        os.makedirs(waveform_dir)\n",
    "\n",
    "    ####### Download data ########\n",
    "    client = Client(config[\"client\"])\n",
    "    fname_list = [\"fname\"]\n",
    "\n",
    "    def download(i):\n",
    "        #     for i in idx:\n",
    "        starttime = obspy.UTCDateTime(starttimes[i])\n",
    "        endtime = starttime + interval\n",
    "        fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "        if not upload_minio:\n",
    "            if os.path.exists(os.path.join(waveform_dir, fname)):\n",
    "                print(f\"{fname} exists\")\n",
    "                fname_list.append(fname)\n",
    "                return\n",
    "        else:\n",
    "            try:\n",
    "                minioClient.fget_object(\n",
    "                    bucket_name,\n",
    "                    os.path.join(config[\"region\"], fname),\n",
    "                    os.path.join(waveform_dir, fname),\n",
    "                )\n",
    "                print(\n",
    "                    f\"{bucket_name}/{os.path.join(config['region'], fname)} download to {os.path.join(waveform_dir, fname)}\"\n",
    "                )\n",
    "                fname_list.append(fname)\n",
    "                return\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "\n",
    "        max_retry = 10\n",
    "        stream = obspy.Stream()\n",
    "        print(f\"{fname} download starts\")\n",
    "        num_sta = 0\n",
    "        for network in stations:\n",
    "            for station in network:\n",
    "                print(f\"********{network.code}.{station.code}********\")\n",
    "                retry = 0\n",
    "                while retry < max_retry:\n",
    "                    try:\n",
    "                        tmp = client.get_waveforms(\n",
    "                            network.code,\n",
    "                            station.code,\n",
    "                            \"*\",\n",
    "                            config[\"channels\"],\n",
    "                            starttime,\n",
    "                            endtime,\n",
    "                        )\n",
    "                        #  for trace in tmp:\n",
    "                        #      if trace.stats.sampling_rate != 100:\n",
    "                        #          print(trace)\n",
    "                        #          trace = trace.interpolate(100, method=\"linear\")\n",
    "                        #      trace = trace.detrend(\"spline\", order=2, dspline=5*trace.stats.sampling_rate)\n",
    "                        #      stream.append(trace)\n",
    "                        stream += tmp\n",
    "                        num_sta += len(tmp)\n",
    "                        break\n",
    "                    except Exception as err:\n",
    "                        print(\"Error {}.{}: {}\".format(network.code, station.code, err))\n",
    "                        message = \"No data available for request.\"\n",
    "                        if str(err)[: len(message)] == message:\n",
    "                            break\n",
    "                        retry += 1\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "                if retry == max_retry:\n",
    "                    print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code}\")\n",
    "\n",
    "        if len(stream) > 0:\n",
    "            # stream = stream.merge(fill_value=0)\n",
    "            # stream = stream.trim(starttime, endtime, pad=True, fill_value=0)\n",
    "            stream.write(os.path.join(waveform_dir, fname))\n",
    "            print(f\"{fname} download succeeds\")\n",
    "            # if upload_minio:\n",
    "            #     minioClient.fput_object(bucket_name, os.path.join(config['region'], fname), os.path.join(waveform_dir, fname))\n",
    "            #     print(f\"{fname} upload to minio {os.path.join(config['region'], fname)}\")\n",
    "        else:\n",
    "            print(f\"{fname} empty data\")\n",
    "        lock.acquire()\n",
    "        fname_list.append(fname)\n",
    "        lock.release()\n",
    "\n",
    "    threads = []\n",
    "    MAX_THREADS = 2\n",
    "    # MAX_THREADS = 1\n",
    "    for ii, i in enumerate(idx):\n",
    "        t = threading.Thread(target=download, args=(i,))\n",
    "        t.start()\n",
    "        time.sleep(1)\n",
    "        threads.append(t)\n",
    "        if ii % MAX_THREADS == MAX_THREADS - 1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    with open(fname_csv, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(fname_list))\n",
    "\n",
    "    return waveform_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.180912Z",
     "iopub.status.busy": "2022-08-11T15:42:43.180716Z",
     "iopub.status.idle": "2022-08-11T15:42:43.183361Z",
     "shell.execute_reply": "2022-08-11T15:42:43.183089Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.180896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    waveform_path = download_waveform(\n",
    "        0,\n",
    "        root_dir(\"index.json\"),\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"datetimes.json\"),\n",
    "        root_dir(\"stations.pkl\"),\n",
    "        root_dir(\"fname.csv\"),\n",
    "        data_path=root_dir(\"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.184062Z",
     "iopub.status.busy": "2022-08-11T15:42:43.183802Z",
     "iopub.status.idle": "2022-08-11T15:42:43.202793Z",
     "shell.execute_reply": "2022-08-11T15:42:43.202417Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.184045Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_waveform_op = comp.func_to_container_op(\n",
    "    download_waveform,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"obspy\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_waveform(\n",
    "    i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    datetime_json: InputPath(\"json\"),\n",
    "    data_path: str,\n",
    "    bucket_name: str = \"waveforms\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> int:\n",
    "\n",
    "    import json\n",
    "    import pickle\n",
    "    import time\n",
    "    import threading\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    waveform_dir = Path(data_path, config[\"region\"], \"waveforms\")\n",
    "    if not waveform_dir.exists():\n",
    "        waveform_dir.mkdir(parents=True)\n",
    "\n",
    "    status = -1\n",
    "    max_retry = 10\n",
    "    retry_rsync = 0\n",
    "\n",
    "    while (status != 0) and (retry_rsync < max_retry):\n",
    "        status = os.system(\n",
    "            f\"rsync -av zhuwq@wintermute:/atomic-data/zhuwq/{config['region']}/{i:03d}/ {data_path}/{config['region']}/waveforms/\"\n",
    "        )\n",
    "        retry_rsync += 1\n",
    "        if status != 0:\n",
    "            time.sleep(5)\n",
    "\n",
    "    if status != 0:\n",
    "        print(\n",
    "            f\"Failed: rsync -av zhuwq@wintermute:/atomic-data/zhuwq/{config['region']}/{i:03d}/ {data_path}/{config['region']}/waveforms/\"\n",
    "        )\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_waveform_op = comp.func_to_container_op(\n",
    "    resume_waveform,\n",
    "    base_image=\"zhuwq0/waveform-env:1.1\",\n",
    "    # base_image='python:3.8',\n",
    "    # packages_to_install=[\"obspy\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_waveform(\n",
    "    i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    datetime_json: InputPath(\"json\"),\n",
    "    data_path: str,\n",
    "    bucket_name: str = \"waveforms\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> int:\n",
    "\n",
    "    import json\n",
    "    import pickle\n",
    "    import time\n",
    "    import threading\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    waveform_dir = Path(data_path, config[\"region\"], \"waveforms\")\n",
    "    mseeds = waveform_dir.glob(\"*.mseed\")\n",
    "\n",
    "    status = -1\n",
    "    max_retry = 10\n",
    "    retry_rsync = 0\n",
    "    while (status != 0) and (retry_rsync < max_retry):\n",
    "        status = os.system(f\"ssh zhuwq@wintermute mkdir -p /atomic-data/zhuwq/{config['region']}/{i:03d}/\")\n",
    "        retry_rsync += 1\n",
    "        if status != 0:\n",
    "            time.sleep(5)\n",
    "\n",
    "    if status != 0:\n",
    "        print(f\"Failed: ssh zhuwq@wintermute mkdir -p /atomic-data/zhuwq/{config['region']}/{i:03d}/\")\n",
    "\n",
    "    status = -1\n",
    "    max_retry = 10\n",
    "    retry_rsync = 0\n",
    "\n",
    "    while (status != 0) and (retry_rsync < max_retry):\n",
    "        status = os.system(\n",
    "            f\"rsync -av {data_path}/{config['region']}/waveforms/ zhuwq@wintermute:/atomic-data/zhuwq/{config['region']}/{i:03d}/\"\n",
    "        )\n",
    "        retry_rsync += 1\n",
    "        if status != 0:\n",
    "            time.sleep(5)\n",
    "\n",
    "    if status != 0:\n",
    "        print(\n",
    "            f\"Failed: rsync -av {data_path}/{config['region']}/waveforms/ zhuwq@wintermute:/atomic-data/zhuwq/{config['region']}/{i:03d}/\"\n",
    "        )\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_waveform_op = comp.func_to_container_op(\n",
    "    save_waveform,\n",
    "    base_image=\"zhuwq0/waveform-env:1.1\",\n",
    "    # base_image='python:3.8',\n",
    "    # packages_to_install=[\"obspy\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run PhaseNet to pick P/S picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.203346Z",
     "iopub.status.busy": "2022-08-11T15:42:43.203230Z",
     "iopub.status.idle": "2022-08-11T15:42:43.206297Z",
     "shell.execute_reply": "2022-08-11T15:42:43.206079Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.203331Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phasenet_op(data_path: str, data_list: str, stations: str):\n",
    "\n",
    "    return dsl.ContainerOp(\n",
    "        name=\"PhaseNet Picking\",\n",
    "        image=\"zhuwq0/phasenet-api:1.0\",\n",
    "        command=[\"python\"],\n",
    "        arguments=[\n",
    "            \"phasenet/predict.py\",\n",
    "            \"--model\",\n",
    "            \"model/190703-214543\",\n",
    "            \"--data_dir\",\n",
    "            data_path,\n",
    "            \"--data_list\",\n",
    "            dsl.InputArgumentPath(data_list),\n",
    "            \"--stations\",\n",
    "            dsl.InputArgumentPath(stations),\n",
    "            # '--result_dir', \"results\",\n",
    "            \"--format\",\n",
    "            \"mseed_array\",\n",
    "            \"--amplitude\",\n",
    "        ],\n",
    "        file_outputs={\"picks\": \"/opt/results/picks.csv\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.206789Z",
     "iopub.status.busy": "2022-08-11T15:42:43.206677Z",
     "iopub.status.idle": "2022-08-11T15:42:43.209109Z",
     "shell.execute_reply": "2022-08-11T15:42:43.208840Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.206774Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "if run_local:\n",
    "    command = f\"python ../PhaseNet/phasenet/predict.py --model=../PhaseNet/model/190703-214543 --data_dir={root_dir(root_dir('waveforms'))} --data_list={root_dir('fname.csv')} --stations={root_dir('stations.json')} --result_dir={root_dir('phasenet')} --format=mseed_array --amplitude\"# --upload_waveform\"\n",
    "    print(command)\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run GaMMA to associate P/S picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.209711Z",
     "iopub.status.busy": "2022-08-11T15:42:43.209601Z",
     "iopub.status.idle": "2022-08-11T15:42:43.224591Z",
     "shell.execute_reply": "2022-08-11T15:42:43.224320Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.209696Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gamma(\n",
    "    node_i: int,\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    pick_csv: InputPath(\"csv\"),\n",
    "    station_json: InputPath(\"json\"),\n",
    "    gamma_catalog_csv: OutputPath(str),\n",
    "    gamma_pick_csv: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    ") -> str:\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from gamma.utils import association, convert_picks_csv, from_seconds\n",
    "    from pyproj import Proj\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "    if not os.path.exists(catalog_dir):\n",
    "        os.makedirs(catalog_dir)\n",
    "\n",
    "    ## read config\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[node_i]\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    ## read picks\n",
    "    # picks = pd.read_json(pick_json)\n",
    "    picks = pd.read_csv(pick_csv, parse_dates=[\"begin_time\", \"phase_time\"])\n",
    "    picks[\"id\"] = picks[\"station_id\"]\n",
    "    picks[\"timestamp\"] = picks[\"phase_time\"]\n",
    "    picks[\"amp\"] = picks[\"phase_amp\"]\n",
    "    picks[\"type\"] = picks[\"phase_type\"]\n",
    "    picks[\"prob\"] = picks[\"phase_score\"]\n",
    "\n",
    "    ## read stations\n",
    "    # stations = pd.read_csv(station_csv, delimiter=\"\\t\")\n",
    "    with open(station_json, \"r\") as fp:\n",
    "        stations = json.load(fp)\n",
    "    stations = pd.DataFrame.from_dict(stations, orient=\"index\")\n",
    "    # stations = stations.rename(columns={\"station\": \"id\"})\n",
    "    stations[\"id\"] = stations.index\n",
    "    proj = Proj(f\"+proj=sterea +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\n",
    "    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n",
    "        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    "    )\n",
    "    stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x / 1e3)\n",
    "\n",
    "    ## setting GMMA configs\n",
    "    config[\"use_dbscan\"] = True\n",
    "    config[\"use_amplitude\"] = True\n",
    "    config[\"method\"] = \"BGMM\"\n",
    "    if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n",
    "        config[\"oversample_factor\"] = 4\n",
    "    if config[\"method\"] == \"GMM\":  ## GaussianMixture\n",
    "        config[\"oversample_factor\"] = 1\n",
    "\n",
    "    # Earthquake location\n",
    "    config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n",
    "    config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.73}\n",
    "    config[\"x(km)\"] = (np.array(config[\"xlim_degree\"]) - np.array(config[\"center\"][0])) * config[\"degree2km\"]\n",
    "    config[\"y(km)\"] = (np.array(config[\"ylim_degree\"]) - np.array(config[\"center\"][1])) * config[\"degree2km\"]\n",
    "    config[\"z(km)\"] = (0, 60)\n",
    "    config[\"bfgs_bounds\"] = (\n",
    "        (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n",
    "        (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n",
    "        (0, config[\"z(km)\"][1] + 1),  # z\n",
    "        (None, None),  # t\n",
    "    )\n",
    "\n",
    "    # DBSCAN\n",
    "    config[\"dbscan_eps\"] = 10  # second\n",
    "    config[\"dbscan_min_samples\"] = 3  ## see DBSCAN\n",
    "\n",
    "    # Filtering\n",
    "    config[\"min_picks_per_eq\"] = min(10, len(stations) // 2)\n",
    "    config[\"min_p_picks_per_eq\"] = 0\n",
    "    config[\"min_s_picks_per_eq\"] = 0\n",
    "    config[\"max_sigma11\"] = 2.0  # s\n",
    "    config[\"max_sigma22\"] = 2.0  # m/s\n",
    "    config[\"max_sigma12\"] = 1.0  # covariance\n",
    "\n",
    "    # if use amplitude\n",
    "    if config[\"use_amplitude\"]:\n",
    "        picks = picks[picks[\"amp\"] != -1]\n",
    "\n",
    "    # print(config)\n",
    "    for k, v in config.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    ## run GMMA association\n",
    "    event_idx0 = 1\n",
    "    assignments = []\n",
    "    catalogs, assignments = association(picks, stations, config, event_idx0, method=config[\"method\"])\n",
    "    event_idx0 += len(catalogs)\n",
    "\n",
    "    if len(catalogs) == 0:\n",
    "        with open(gamma_catalog_csv, \"w\") as fp:\n",
    "            fp.write(\"time,magnitude,longitude,latitude,depth(m),sigma_time,sigma_amp,cov_time_amp,gamma_score,event_index\\n\")\n",
    "        with open(gamma_pick_csv, \"w\") as fp:\n",
    "            fp.write(\"station_id,phase_time,phase_type,phase_score,phase_amp,gamma_score,event_index\\n\")\n",
    "\n",
    "    else:\n",
    "        ## create catalog\n",
    "        catalogs = pd.DataFrame(\n",
    "            catalogs,\n",
    "            columns=[\"time\"]\n",
    "            + config[\"dims\"]\n",
    "            + [\n",
    "                \"magnitude\",\n",
    "                \"sigma_time\",\n",
    "                \"sigma_amp\",\n",
    "                \"cov_time_amp\",\n",
    "                \"event_index\",\n",
    "                \"gamma_score\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        catalogs[[\"longitude\", \"latitude\"]] = catalogs.apply(\n",
    "            lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)),\n",
    "            axis=1,\n",
    "        )\n",
    "        catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x * 1e3)\n",
    "\n",
    "        catalogs.sort_values(by=[\"time\"], inplace=True)\n",
    "        with open(gamma_catalog_csv, \"w\") as fp:\n",
    "            catalogs.to_csv(\n",
    "                fp,\n",
    "                # sep=\"\\t\",\n",
    "                index=False,\n",
    "                float_format=\"%.3f\",\n",
    "                date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "                columns=[\n",
    "                    \"time\",\n",
    "                    \"magnitude\",\n",
    "                    \"longitude\",\n",
    "                    \"latitude\",\n",
    "                    \"depth(m)\",\n",
    "                    \"sigma_time\",\n",
    "                    \"sigma_amp\",\n",
    "                    \"cov_time_amp\",\n",
    "                    \"gamma_score\",\n",
    "                    \"event_index\",\n",
    "                ],\n",
    "            )\n",
    "        # catalogs = catalogs[\n",
    "        #     ['time', 'magnitude', 'longitude', 'latitude', 'depth(m)', 'sigma_time', 'sigma_amp']\n",
    "        # ]\n",
    "\n",
    "        ## add assignment to picks\n",
    "        assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n",
    "        picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n",
    "        picks.sort_values(by=[\"timestamp\"], inplace=True)\n",
    "        with open(gamma_pick_csv, \"w\") as fp:\n",
    "            picks.to_csv(\n",
    "                fp,\n",
    "                # sep=\"\\t\",\n",
    "                index=False,\n",
    "                date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "                columns=[\n",
    "                    \"station_id\",\n",
    "                    \"phase_time\",\n",
    "                    \"phase_type\",\n",
    "                    \"phase_score\",\n",
    "                    \"phase_amp\",\n",
    "                    \"gamma_score\",\n",
    "                    \"event_index\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/gamma/catalog_{node_i:03d}.csv\",\n",
    "            gamma_catalog_csv,\n",
    "        )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/gamma/picks_{node_i:03d}.csv\",\n",
    "            gamma_pick_csv,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    ## upload to mongodb\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "\n",
    "        username = \"root\"\n",
    "        password = \"quakeflow123\"\n",
    "        client = MongoClient(f\"mongodb://{username}:{password}@127.0.0.1:27017\")\n",
    "        db = client[\"quakeflow\"]\n",
    "        collection = db[\"waveform\"]\n",
    "        for i, p in tqdm(picks.iterrows(), desc=\"Uploading to mongodb\"):\n",
    "            collection.update(\n",
    "                {\"_id\": f\"{p['station_id']}_{p['timestamp'].isoformat(timespec='milliseconds')}_{p['type']}\"},\n",
    "                {\"$set\": {\"event_index\": p[\"event_index\"]}},\n",
    "            )\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access mongodb service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    return f\"catalog_{node_i:03d}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.225205Z",
     "iopub.status.busy": "2022-08-11T15:42:43.225080Z",
     "iopub.status.idle": "2022-08-11T15:42:43.227609Z",
     "shell.execute_reply": "2022-08-11T15:42:43.227339Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.225188Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    catalog = gamma(\n",
    "        0,\n",
    "        root_dir(\"index.json\"),\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"phasenet/picks.csv\"),\n",
    "        root_dir(\"stations.json\"),\n",
    "        root_dir(\"gamma_catalog.csv\"),\n",
    "        root_dir(\"gamma_picks.csv\"),\n",
    "        bucket_name=\"catalogs\",\n",
    "        s3_url=\"localhost:9000\",\n",
    "        secure=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.228292Z",
     "iopub.status.busy": "2022-08-11T15:42:43.228076Z",
     "iopub.status.idle": "2022-08-11T15:42:43.257021Z",
     "shell.execute_reply": "2022-08-11T15:42:43.256784Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.228276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma_op = comp.func_to_container_op(\n",
    "    gamma,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"scikit-learn\",\n",
    "        \"tqdm\",\n",
    "        \"minio\",\n",
    "        \"gmma\",\n",
    "        \"pyproj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.257537Z",
     "iopub.status.busy": "2022-08-11T15:42:43.257425Z",
     "iopub.status.idle": "2022-08-11T15:42:43.259721Z",
     "shell.execute_reply": "2022-08-11T15:42:43.259471Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.257522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    %run plot_catalog.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge parallel processing on cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.260305Z",
     "iopub.status.busy": "2022-08-11T15:42:43.260193Z",
     "iopub.status.idle": "2022-08-11T15:42:43.268304Z",
     "shell.execute_reply": "2022-08-11T15:42:43.268088Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.260290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " def merge_catalog(\n",
    "    index_json: InputPath(\"json\"),\n",
    "    config_json: InputPath(\"json\"),\n",
    "    catalog_csv: OutputPath(str),\n",
    "    picks_csv: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    from glob import glob\n",
    "\n",
    "    import pandas as pd\n",
    "    from minio import Minio\n",
    "\n",
    "    minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "\n",
    "    with open(index_json, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "        \n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    objects = minioClient.list_objects(bucket_name, prefix=f\"{config['region']}/gamma\", recursive=True)\n",
    "\n",
    "    tmp_path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "    for obj in objects:\n",
    "        print(obj._object_name)\n",
    "        minioClient.fget_object(bucket_name, obj._object_name, tmp_path(obj._object_name.split(\"/\")[-1]))\n",
    "\n",
    "    # files_catalog = sorted(glob(tmp_path(\"catalog_*.csv\")))\n",
    "    # files_picks = sorted(glob(tmp_path(\"picks_*.csv\")))\n",
    "    files_catalog = [tmp_path(f\"catalog_{node_i:03d}.csv\") for node_i in range(len(index))]\n",
    "    files_picks = [tmp_path(f\"picks_{node_i:03d}.csv\") for node_i in range(len(index))]\n",
    "    print(f\"Merge catalog: {files_catalog}\")\n",
    "    print(f\"Merge picks: {files_picks}\")\n",
    "\n",
    "    if len(files_catalog) > 0:\n",
    "        catalog_list = []\n",
    "        for f in files_catalog:\n",
    "            if not os.path.exists(f):\n",
    "                continue\n",
    "            tmp = pd.read_csv(f, dtype=str)\n",
    "            tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "            catalog_list.append(tmp)\n",
    "        merged_catalog = pd.concat(catalog_list).sort_values(by=\"time\")\n",
    "        merged_catalog[\"match_id\"] = merged_catalog.apply(lambda x: f'{x[\"event_index\"]}_{x[\"file_index\"]}', axis=1)\n",
    "        merged_catalog.sort_values(by=\"time\", inplace=True, ignore_index=True)\n",
    "        merged_catalog.drop(columns=[\"event_index\", \"file_index\"], inplace=True)\n",
    "        merged_catalog[\"event_index\"] = merged_catalog.index.values + 1\n",
    "        mapping = dict(zip(merged_catalog[\"match_id\"], merged_catalog[\"event_index\"]))\n",
    "        merged_catalog.drop(columns=[\"match_id\"], inplace=True)\n",
    "        merged_catalog.to_csv(catalog_csv, index=False)\n",
    "        del merged_catalog\n",
    "\n",
    "        pick_list = []\n",
    "        for f in files_picks:\n",
    "            if not os.path.exists(f):\n",
    "                continue\n",
    "            tmp = pd.read_csv(f, dtype=str)\n",
    "            tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "            pick_list.append(tmp)\n",
    "        merged_picks = pd.concat(pick_list).sort_values(by=\"phase_time\")\n",
    "        merged_picks[\"match_id\"] = merged_picks.apply(lambda x: f'{x[\"event_index\"]}_{x[\"file_index\"]}', axis=1)\n",
    "        merged_picks.drop(columns=[\"event_index\", \"file_index\"], inplace=True)\n",
    "        merged_picks[\"event_index\"] = merged_picks[\"match_id\"].apply(lambda x: mapping[x] if x in mapping else -1)\n",
    "        merged_picks.drop(columns=[\"match_id\"], inplace=True)\n",
    "        merged_picks.to_csv(picks_csv, index=False)\n",
    "        del merged_picks\n",
    "        \n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/gamma_catalog.csv\",\n",
    "            catalog_csv,\n",
    "        )\n",
    "        \n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/gamma_picks.csv\",\n",
    "            picks_csv,\n",
    "        )\n",
    "\n",
    "    # else:\n",
    "    #     with open(catalog_csv, \"w\") as fout:\n",
    "    #         pass\n",
    "    #     print(\"No catalog.csv found!\")\n",
    "    #     with open(picks_csv, \"w\") as fout:\n",
    "    #         pass\n",
    "    #     print(\"No picks.csv found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.269617Z",
     "iopub.status.busy": "2022-08-11T15:42:43.269540Z",
     "iopub.status.idle": "2022-08-11T15:42:43.287964Z",
     "shell.execute_reply": "2022-08-11T15:42:43.287552Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.269607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_op = comp.func_to_container_op(\n",
    "    merge_catalog,\n",
    "    # base_image='zhuwq0/quakeflow-env:latest',\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. HypoDD earthquake relocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and compile HypoDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.288365Z",
     "iopub.status.busy": "2022-08-11T15:42:43.288297Z",
     "iopub.status.idle": "2022-08-11T15:42:43.290807Z",
     "shell.execute_reply": "2022-08-11T15:42:43.290590Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.288356Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    if not os.path.exists(\"HYPODD\"):\n",
    "        os.system(\"wget -O HYPODD_1.3.tar.gz http://www.ldeo.columbia.edu/~felixw/HYPODD/HYPODD_1.3.tar.gz\")\n",
    "        os.system(\"tar -xf HYPODD_1.3.tar.gz\")\n",
    "        os.system(\"ln -s $(which gfortran) ./HYPODD/f77\")\n",
    "        os.system(\"ln -s $(which gfortran) ./HYPODD/g77\")\n",
    "        os.environ[\"PATH\"] += os.pathsep + os.getcwd() + \"/HYPODD/\"\n",
    "        os.system(\"make -C HYPODD/src\")\n",
    "    if not os.path.exists(root_dir(\"hypodd\")):\n",
    "        os.mkdir(root_dir(\"hypodd\"))\n",
    "    os.system(f\"cp HYPODD/src/ph2dt/ph2dt {root_dir('hypodd')}/\")\n",
    "    os.system(f\"cp HYPODD/src/hypoDD/hypoDD {root_dir('hypodd')}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Convert station format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.291295Z",
     "iopub.status.busy": "2022-08-11T15:42:43.291139Z",
     "iopub.status.idle": "2022-08-11T15:42:43.295433Z",
     "shell.execute_reply": "2022-08-11T15:42:43.295218Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.291285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_station(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    station_json: InputPath(\"json\"),\n",
    "    hypoinverse_station: OutputPath(str),\n",
    "    hypodd_station: OutputPath(str),\n",
    "    data_path: str = \"./\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    hypodd_path = os.path.join(data_path, \"hypodd\")\n",
    "    if not os.path.exists(hypodd_path):\n",
    "        os.mkdir(hypodd_path)\n",
    "    hypoinv_path = os.path.join(data_path, \"hypoinv\")\n",
    "    if not os.path.exists(hypoinv_path):\n",
    "        os.mkdir(hypoinv_path)\n",
    "\n",
    "    stations = pd.read_json(station_json, orient=\"index\")\n",
    "\n",
    "    converted_hypoinverse = []\n",
    "    converted_hypodd = {}\n",
    "\n",
    "    for sta, row in tqdm(stations.iterrows()):\n",
    "\n",
    "        network_code, station_code, comp_code, channel_code = sta.split(\".\")\n",
    "        station_weight = \" \"\n",
    "        lat_degree = int(row[\"latitude\"])\n",
    "        lat_minute = (row[\"latitude\"] - lat_degree) * 60\n",
    "        north = \"N\" if lat_degree >= 0 else \"S\"\n",
    "        lng_degree = int(row[\"longitude\"])\n",
    "        lng_minute = (row[\"longitude\"] - lng_degree) * 60\n",
    "        west = \"W\" if lng_degree <= 0 else \"E\"\n",
    "        elevation = row[\"elevation(m)\"]\n",
    "        line_hypoinverse = f\"{station_code:<5} {network_code:<2} {comp_code[:-1]:<1}{channel_code:<3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"\n",
    "        converted_hypoinverse.append(line_hypoinverse)\n",
    "\n",
    "        tmp_code = f\"{station_code}{channel_code}\"\n",
    "        converted_hypodd[\n",
    "            f\"{station_code}{channel_code}\"\n",
    "        ] = f\"{tmp_code:<8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"\n",
    "\n",
    "    with open(hypoinverse_station, \"w\") as f:\n",
    "        f.writelines(converted_hypoinverse)\n",
    "\n",
    "    with open(hypodd_station, \"w\") as f:\n",
    "        for k, v in converted_hypodd.items():\n",
    "            f.write(v)\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/hypodd/stations.dat\",\n",
    "            hypodd_station,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.295850Z",
     "iopub.status.busy": "2022-08-11T15:42:43.295777Z",
     "iopub.status.idle": "2022-08-11T15:42:43.297872Z",
     "shell.execute_reply": "2022-08-11T15:42:43.297673Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.295841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    convert_station(\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"stations.json\"),\n",
    "        root_dir(\"hypoinv/stations.dat\"),\n",
    "        root_dir(\"hypodd/stations.dat\"),\n",
    "        root_dir(\"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.298260Z",
     "iopub.status.busy": "2022-08-11T15:42:43.298168Z",
     "iopub.status.idle": "2022-08-11T15:42:43.307008Z",
     "shell.execute_reply": "2022-08-11T15:42:43.306794Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.298251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert_station_op = comp.func_to_container_op(\n",
    "    convert_station,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"tqdm\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split large catalog due to memory and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.307446Z",
     "iopub.status.busy": "2022-08-11T15:42:43.307349Z",
     "iopub.status.idle": "2022-08-11T15:42:43.309961Z",
     "shell.execute_reply": "2022-08-11T15:42:43.309779Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.307435Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_hypodd(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    catalog_csv: InputPath(str),\n",
    ") -> list:\n",
    "\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    events = pd.read_csv(catalog_csv)\n",
    "\n",
    "    if \"MAXEVENT\" in config[\"hypodd\"]:\n",
    "        MAXEVENT = config[\"hypodd\"][\"MAXEVENT\"]\n",
    "    else:\n",
    "        MAXEVENT = 1e4  ## segment by time\n",
    "\n",
    "    MAXEVENT = len(events) // ((len(events) - 1) // MAXEVENT + 1) + 1\n",
    "    num_parallel = int((len(events) - 1) // MAXEVENT + 1)\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.310371Z",
     "iopub.status.busy": "2022-08-11T15:42:43.310275Z",
     "iopub.status.idle": "2022-08-11T15:42:43.312038Z",
     "shell.execute_reply": "2022-08-11T15:42:43.311821Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.310362Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    nodes = split_hypodd(\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"gamma_catalog.csv\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.312414Z",
     "iopub.status.busy": "2022-08-11T15:42:43.312323Z",
     "iopub.status.idle": "2022-08-11T15:42:43.317514Z",
     "shell.execute_reply": "2022-08-11T15:42:43.317317Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.312405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_hypodd_op = comp.func_to_container_op(\n",
    "    split_hypodd,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Convert phase format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.317903Z",
     "iopub.status.busy": "2022-08-11T15:42:43.317834Z",
     "iopub.status.idle": "2022-08-11T15:42:43.323986Z",
     "shell.execute_reply": "2022-08-11T15:42:43.323639Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.317894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_phase(\n",
    "    node_i: int,\n",
    "    config_json: InputPath(\"json\"),\n",
    "    picks_csv: InputPath(str),\n",
    "    catalog_csv: InputPath(str),\n",
    "    hypodd_phase: OutputPath(str),\n",
    "    data_path: str = \"./\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    hypodd_path = os.path.join(data_path, \"hypodd\")\n",
    "    if not os.path.exists(hypodd_path):\n",
    "        os.mkdir(hypodd_path)\n",
    "\n",
    "    picks = pd.read_csv(picks_csv)\n",
    "    events = pd.read_csv(catalog_csv)\n",
    "\n",
    "    if \"MAXEVENT\" in config[\"hypodd\"]:\n",
    "        MAXEVENT = config[\"hypodd\"][\"MAXEVENT\"]\n",
    "    else:\n",
    "        MAXEVENT = 1e4  ## segment by time\n",
    "    MAXEVENT = len(events) // ((len(events) - 1) // MAXEVENT + 1) + 1\n",
    "    num_parallel = int((len(events) - 1) // MAXEVENT + 1)\n",
    "\n",
    "    events.sort_values(\"time\", inplace=True)\n",
    "    events = events.iloc[node_i::num_parallel]\n",
    "    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n",
    "    # output_lines = []\n",
    "    output_file = open(hypodd_phase, \"w\")\n",
    "\n",
    "    picks_by_event = picks.groupby(\"event_index\").groups\n",
    "    # for i in tqdm(range(node_i, len(events), num_parallel)):\n",
    "    #     event = events.iloc[i]\n",
    "    for i, event in events.iterrows():\n",
    "        event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        lat = event[\"latitude\"]\n",
    "        lng = event[\"longitude\"]\n",
    "        dep = event[\"depth(m)\"] / 1e3\n",
    "        mag = event[\"magnitude\"]\n",
    "        EH = 0\n",
    "        EZ = 0\n",
    "        RMS = event[\"sigma_time\"]\n",
    "\n",
    "        year, month, day, hour, min, sec = (\n",
    "            event_time.year,\n",
    "            event_time.month,\n",
    "            event_time.day,\n",
    "            event_time.hour,\n",
    "            event_time.minute,\n",
    "            float(event_time.strftime(\"%S.%f\")),\n",
    "        )\n",
    "        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n",
    "\n",
    "        # output_lines.append(event_line)\n",
    "        output_file.write(event_line)\n",
    "\n",
    "        picks_idx = picks_by_event[event[\"event_index\"]]\n",
    "        for j in picks_idx:\n",
    "            # pick = picks.iloc[j]\n",
    "            pick = picks.loc[j]\n",
    "            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n",
    "            phase_type = pick[\"phase_type\"].upper()\n",
    "            phase_score = pick[\"phase_score\"]\n",
    "            pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n",
    "            tmp_code = f\"{station_code}{channel_code}\"\n",
    "            pick_line = f\"{tmp_code:<7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n",
    "            # output_lines.append(pick_line)\n",
    "            output_file.write(pick_line)\n",
    "\n",
    "    # with open(hypodd_phase, \"w\") as fp:\n",
    "    #     fp.writelines(output_lines)\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        # minioClient.fput_object(\n",
    "        #     bucket_name,\n",
    "        #     f\"{config['region']}/hypodd/phase_{node_i:03d}.pha\",\n",
    "        #     hypodd_phase,\n",
    "        # )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    return hypodd_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.324534Z",
     "iopub.status.busy": "2022-08-11T15:42:43.324342Z",
     "iopub.status.idle": "2022-08-11T15:42:43.326694Z",
     "shell.execute_reply": "2022-08-11T15:42:43.326362Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.324524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    for node_i in nodes:\n",
    "        convert_phase(\n",
    "            node_i,\n",
    "            root_dir(\"config.json\"),\n",
    "            root_dir(\"gamma_picks.csv\"),\n",
    "            root_dir(\"gamma_catalog.csv\"),\n",
    "            root_dir(\"hypodd/hypodd_phase.pha\"),\n",
    "            root_dir(\"\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.327081Z",
     "iopub.status.busy": "2022-08-11T15:42:43.327013Z",
     "iopub.status.idle": "2022-08-11T15:42:43.342216Z",
     "shell.execute_reply": "2022-08-11T15:42:43.341954Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.327072Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert_phase_op = comp.func_to_container_op(\n",
    "    convert_phase,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"tqdm\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Run ph2dt to calculate differential time between phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.342670Z",
     "iopub.status.busy": "2022-08-11T15:42:43.342599Z",
     "iopub.status.idle": "2022-08-11T15:42:43.347476Z",
     "shell.execute_reply": "2022-08-11T15:42:43.347261Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.342660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ph2dt(\n",
    "    node_i: int,\n",
    "    config_json: InputPath(\"json\"),\n",
    "    hypodd_phase: InputPath(str),\n",
    "    station_dat: InputPath(str),\n",
    "    ct_file: OutputPath(str),\n",
    "    hypodd_event: OutputPath(str),\n",
    "    data_path: str = \"./\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    from minio import Minio\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    hypodd_path = os.path.join(data_path, \"hypodd\")\n",
    "    if not os.path.exists(hypodd_path):\n",
    "        os.mkdir(hypodd_path)\n",
    "\n",
    "    # try:\n",
    "    #     minioClient = Minio(s3_url, access_key='minio', secret_key='minio123', secure=secure)\n",
    "    #     minioClient.fget_object(bucket_name, f\"{config['region']}/hypodd_{node_i:03d}.pha\", os.path.join(hypodd_path, f\"hypodd_{node_i:03d}.pha\"))\n",
    "    # except Exception as err:\n",
    "    #     print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "    #     pass\n",
    "\n",
    "    ph2dt = \"\"\"* ph2dt.inp - input control file for program ph2dt\n",
    "* Input station file:\n",
    "stations.dat\n",
    "* Input phase file:\n",
    "hypodd.pha\n",
    "*MINWGHT: min. pick weight allowed [0]\n",
    "*MAXDIST: max. distance in km between event pair and stations [200]\n",
    "*MAXSEP: max. hypocentral separation in km [10]\n",
    "*MAXNGH: max. number of neighbors per event [10]\n",
    "*MINLNK: min. number of links required to define a neighbor [8]\n",
    "*MINOBS: min. number of links per pair saved [8]\n",
    "*MAXOBS: max. number of links per pair saved [20]\n",
    "*MINWGHT MAXDIST MAXSEP MAXNGH MINLNK MINOBS MAXOBS\n",
    "   0      120     10     50     8      8     100\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(hypodd_path, \"ph2dt.inp\"), \"w\") as fp:\n",
    "        fp.writelines(ph2dt)\n",
    "\n",
    "    def copy_file(fp_from, fp_to):\n",
    "        with open(fp_from, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        with open(fp_to, \"w\") as fp:\n",
    "            fp.writelines(lines)\n",
    "\n",
    "    copy_file(hypodd_phase, os.path.join(hypodd_path, \"hypodd.pha\"))\n",
    "    copy_file(station_dat, os.path.join(hypodd_path, \"stations.dat\"))\n",
    "\n",
    "    PH2DT_CMD = f\"cd {hypodd_path} && ./ph2dt ph2dt.inp\"\n",
    "    print(PH2DT_CMD)\n",
    "    if os.system(PH2DT_CMD) != 0:\n",
    "        raise (\"{PH2DT_CMD}\" + \" failed!\")\n",
    "\n",
    "    copy_file(os.path.join(hypodd_path, \"dt.ct\"), ct_file)\n",
    "    copy_file(os.path.join(hypodd_path, \"event.sel\"), hypodd_event)\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        # minioClient.fput_object(\n",
    "        #     bucket_name,\n",
    "        #     f\"{config['region']}/hypodd/dt_{node_i:03d}.ct\",\n",
    "        #     ct_file,\n",
    "        # )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/hypodd/event_{node_i:03d}.sel\",\n",
    "            hypodd_event,\n",
    "        )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass\n",
    "\n",
    "    # try:\n",
    "    #     minioClient.fput_object(\n",
    "    #         bucket_name,\n",
    "    #         f\"{config['region']}/dt_{node_i:03d}.ct\",\n",
    "    #         f\"{os.path.join(hypodd_path, 'dt.ct')}\",\n",
    "    #     )\n",
    "    #     minioClient.fput_object(\n",
    "    #         bucket_name,\n",
    "    #         f\"{config['region']}/event_{node_i:03d}.dat\",\n",
    "    #         f\"{os.path.join(hypodd_path, 'event.dat')}\",\n",
    "    #     )\n",
    "    #     minioClient.fput_object(\n",
    "    #         bucket_name,\n",
    "    #         f\"{config['region']}/event_{node_i:03d}.sel\",\n",
    "    #         f\"{os.path.join(hypodd_path, 'event.sel')}\",\n",
    "    #     )\n",
    "    # except Exception as err:\n",
    "    #     print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "    #     pass\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.347866Z",
     "iopub.status.busy": "2022-08-11T15:42:43.347786Z",
     "iopub.status.idle": "2022-08-11T15:42:43.350092Z",
     "shell.execute_reply": "2022-08-11T15:42:43.349843Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.347857Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    ph2dt(\n",
    "        0,\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"hypodd/hypodd_phase.pha\"),\n",
    "        root_dir(\"hypodd/stations.dat\"),\n",
    "        root_dir(\"hypodd/dt.ct\"),\n",
    "        root_dir(\"hypodd/event.sel\"),\n",
    "        root_dir(\"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.350518Z",
     "iopub.status.busy": "2022-08-11T15:42:43.350398Z",
     "iopub.status.idle": "2022-08-11T15:42:43.361412Z",
     "shell.execute_reply": "2022-08-11T15:42:43.361184Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.350509Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ph2dt_op = comp.func_to_container_op(ph2dt, base_image=\"zhuwq0/hypodd-api:1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Run HypoDD re-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.361862Z",
     "iopub.status.busy": "2022-08-11T15:42:43.361761Z",
     "iopub.status.idle": "2022-08-11T15:42:43.366876Z",
     "shell.execute_reply": "2022-08-11T15:42:43.366650Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.361852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hypodd_ct(\n",
    "    node_i: int,\n",
    "    config_json: InputPath(\"json\"),\n",
    "    ct_file: InputPath(str),\n",
    "    event: InputPath(str),\n",
    "    station: InputPath(str),\n",
    "    catalog_txt: OutputPath(str),\n",
    "    data_path: str = \"./\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    hypodd_path = os.path.join(data_path, \"hypodd\")\n",
    "\n",
    "    ct_inp = \"\"\"* RELOC.INP:\n",
    "*--- input file selection\n",
    "* cross correlation diff times:\n",
    "\n",
    "*\n",
    "*catalog P diff times:\n",
    "dt.ct\n",
    "*\n",
    "* event file:\n",
    "event.sel\n",
    "*\n",
    "* station file:\n",
    "stations.dat\n",
    "*\n",
    "*--- output file selection\n",
    "* original locations:\n",
    "hypodd.loc\n",
    "* relocations:\n",
    "hypodd.reloc\n",
    "* station information:\n",
    "hypodd.sta\n",
    "* residual information:\n",
    "hypodd.res\n",
    "* source paramater information:\n",
    "hypodd.src\n",
    "*\n",
    "*--- data type selection: \n",
    "* IDAT:  0 = synthetics; 1= cross corr; 2= catalog; 3= cross & cat \n",
    "* IPHA: 1= P; 2= S; 3= P&S\n",
    "* DIST:max dist [km] between cluster centroid and station \n",
    "* IDAT   IPHA   DIST\n",
    "    2     3     120\n",
    "*\n",
    "*--- event clustering:\n",
    "* OBSCC:    min # of obs/pair for crosstime data (0= no clustering)\n",
    "* OBSCT:    min # of obs/pair for network data (0= no clustering)\n",
    "* OBSCC  OBSCT    \n",
    "     0     8        \n",
    "*\n",
    "*--- solution control:\n",
    "* ISTART:  \t1 = from single source; 2 = from network sources\n",
    "* ISOLV:\t1 = SVD, 2=lsqr\n",
    "* NSET:      \tnumber of sets of iteration with specifications following\n",
    "*  ISTART  ISOLV  NSET\n",
    "    2        2      4\n",
    "*\n",
    "*--- data weighting and re-weighting: \n",
    "* NITER: \t\tlast iteration to used the following weights\n",
    "* WTCCP, WTCCS:\t\tweight cross P, S \n",
    "* WTCTP, WTCTS:\t\tweight catalog P, S \n",
    "* WRCC, WRCT:\t\tresidual threshold in sec for cross, catalog data \n",
    "* WDCC, WDCT:  \t\tmax dist [km] between cross, catalog linked pairs\n",
    "* DAMP:    \t\tdamping (for lsqr only) \n",
    "*       ---  CROSS DATA ----- ----CATALOG DATA ----\n",
    "* NITER WTCCP WTCCS WRCC WDCC WTCTP WTCTS WRCT WDCT DAMP\n",
    "   4     -9     -9   -9    -9   1     1      8   -9  70 \n",
    "   4     -9     -9   -9    -9   1     1      6    4  70 \n",
    "   4     -9     -9   -9    -9   1    0.8     4    2  70 \n",
    "   4     -9     -9   -9    -9   1    0.8     3    2  70 \n",
    "*\n",
    "*--- 1D model:\n",
    "* NLAY:\t\tnumber of model layers  \n",
    "* RATIO:\tvp/vs ratio \n",
    "* TOP:\t\tdepths of top of layer (km) \n",
    "* VEL: \t\tlayer velocities (km/s)\n",
    "* NLAY  RATIO \n",
    "   12     1.82\n",
    "* TOP \n",
    "0.0 1.0 3.0 5.0 7.0 9.0 11.0 13.0 17.0 21.0 31.00 31.10\n",
    "* VEL\n",
    "5.30 5.65 5.93 6.20 6.20 6.20 6.20 6.20 6.20 6.20 7.50 8.11\n",
    "*\n",
    "*--- event selection:\n",
    "* CID: \tcluster to be relocated (0 = all)\n",
    "* ID:\tcuspids of event to be relocated (8 per line)\n",
    "* CID    \n",
    "    0      \n",
    "* ID\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(hypodd_path, \"ct.inp\"), \"w\") as fp:\n",
    "        fp.writelines(ct_inp)\n",
    "\n",
    "    def copy_file(fp_from, fp_to):\n",
    "        with open(fp_from, \"r\") as fp:\n",
    "            lines = fp.readlines()\n",
    "        with open(fp_to, \"w\") as fp:\n",
    "            fp.writelines(lines)\n",
    "\n",
    "    copy_file(ct_file, os.path.join(hypodd_path, \"dt.ct\"))\n",
    "    copy_file(event, os.path.join(hypodd_path, \"event.sel\"))\n",
    "    copy_file(station, os.path.join(hypodd_path, \"stations.dat\"))\n",
    "\n",
    "    # os.system(f\"cat {ct_file}\")\n",
    "    # os.system(f\"cat {event}\")\n",
    "    # os.system(f\"cat {station}\")\n",
    "\n",
    "    HYPODD_CMD = f\"cd {hypodd_path} && ./hypoDD ct.inp\"\n",
    "    print(HYPODD_CMD)\n",
    "    if os.system(HYPODD_CMD) != 0:\n",
    "        raise (\"{HYPODD_CMD}\" + \" failed!\")\n",
    "\n",
    "    copy_file(os.path.join(hypodd_path, \"hypodd.reloc\"), catalog_txt)\n",
    "\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/hypodd/hypodd_ct_{node_i:03d}.reloc\",\n",
    "            catalog_txt,\n",
    "        )\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.367464Z",
     "iopub.status.busy": "2022-08-11T15:42:43.367254Z",
     "iopub.status.idle": "2022-08-11T15:42:43.369716Z",
     "shell.execute_reply": "2022-08-11T15:42:43.369497Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.367448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    hypodd_ct(\n",
    "        0,\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"hypodd/dt.ct\"),\n",
    "        root_dir(\"hypodd/event.sel\"),\n",
    "        root_dir(\"hypodd/stations.dat\"),\n",
    "        root_dir(\"hypodd_ct_catalog.txt\"),\n",
    "        root_dir(\"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.370228Z",
     "iopub.status.busy": "2022-08-11T15:42:43.370081Z",
     "iopub.status.idle": "2022-08-11T15:42:43.383077Z",
     "shell.execute_reply": "2022-08-11T15:42:43.382825Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.370214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypodd_ct_op = comp.func_to_container_op(hypodd_ct, base_image=\"zhuwq0/hypodd-api:1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.383583Z",
     "iopub.status.busy": "2022-08-11T15:42:43.383476Z",
     "iopub.status.idle": "2022-08-11T15:42:43.387261Z",
     "shell.execute_reply": "2022-08-11T15:42:43.387048Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.383569Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    from datetime import datetime\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    catalog_hypodd = pd.read_csv(\n",
    "        root_dir(f\"hypodd/hypodd.reloc\"),\n",
    "        sep=\"\\s+\",\n",
    "        names=[\n",
    "            \"ID\",\n",
    "            \"LAT\",\n",
    "            \"LON\",\n",
    "            \"DEPTH\",\n",
    "            \"X\",\n",
    "            \"Y\",\n",
    "            \"Z\",\n",
    "            \"EX\",\n",
    "            \"EY\",\n",
    "            \"EZ\",\n",
    "            \"YR\",\n",
    "            \"MO\",\n",
    "            \"DY\",\n",
    "            \"HR\",\n",
    "            \"MI\",\n",
    "            \"SC\",\n",
    "            \"MAG\",\n",
    "            \"NCCP\",\n",
    "            \"NCCS\",\n",
    "            \"NCTP\",\n",
    "            \"NCTS\",\n",
    "            \"RCC\",\n",
    "            \"RCT\",\n",
    "            \"CID\",\n",
    "        ],\n",
    "    )\n",
    "    catalog_hypodd[\"time\"] = catalog_hypodd.apply(\n",
    "        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{min(x[\"SC\"], 59.999):05.3f}',\n",
    "        axis=1,\n",
    "    )\n",
    "    catalog_hypodd[\"time\"] = catalog_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "    plt.figure()\n",
    "    plt.plot(catalog_hypodd[\"LON\"], catalog_hypodd[\"LAT\"], \".\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.387833Z",
     "iopub.status.busy": "2022-08-11T15:42:43.387723Z",
     "iopub.status.idle": "2022-08-11T15:42:43.397330Z",
     "shell.execute_reply": "2022-08-11T15:42:43.397111Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.387817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_correlation(\n",
    "    ct_file: InputPath(str),\n",
    "    catalog_file: InputPath(str),\n",
    "    picks_file: InputPath(str),\n",
    "    cc_file: OutputPath(str),\n",
    "):\n",
    "\n",
    "    import time\n",
    "    from multiprocessing import Manager, Process\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pymongo import MongoClient\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    catalog = pd.read_csv(\n",
    "        catalog_file,\n",
    "        sep=\"\\t\",\n",
    "        parse_dates=[\"time\"],\n",
    "        index_col=[\"event_idx\"],\n",
    "        dtype={\"event_idx\": str},\n",
    "    )\n",
    "    picks = pd.read_csv(picks_file, sep=\"\\t\", parse_dates=[\"timestamp\"], dtype={\"event_idx\": str})\n",
    "    picks[\"station\"] = picks[\"id\"].apply(lambda x: x.split(\".\")[1] + x.split(\".\")[3])\n",
    "    picks = picks.set_index([\"event_idx\", \"station\", \"type\"])\n",
    "    picks = picks.sort_index()\n",
    "\n",
    "    pick_index = 100\n",
    "    lo = pick_index - 50\n",
    "    hi = pick_index + 100\n",
    "    dt = 0.01\n",
    "\n",
    "    ct_dict = Manager().dict()\n",
    "    cc_dict = Manager().dict()\n",
    "    with open(ct_file) as fct:\n",
    "        meta = fct.readlines()\n",
    "        for i, line in enumerate(meta):\n",
    "            if line[0] == \"#\":\n",
    "                if i > 0:\n",
    "                    ct_dict[key] = value\n",
    "                key = line\n",
    "                value = []\n",
    "                continue\n",
    "            value.append(line)\n",
    "        ct_dict[key] = value\n",
    "    keys = sorted(list(ct_dict.keys()))\n",
    "\n",
    "    def calc_cross_correlation(keys, ct_dict, cc_dict):\n",
    "        username = \"root\"\n",
    "        password = \"quakeflow123\"\n",
    "        # client = MongoClient(f\"mongodb://{username}:{password}@127.0.0.1:27017\")\n",
    "        client = MongoClient(f\"mongodb://{username}:{password}@quakeflow-mongodb.default.svc.cluster.local:27017\")\n",
    "        db = client[\"quakeflow\"]\n",
    "        collection = db[\"waveform\"]\n",
    "        # normalize = lambda x: (x - np.mean(x, axis=0, keepdims=True)) / np.std(x, axis=0, keepdims=True)\n",
    "\n",
    "        for key in keys:\n",
    "            tmp = key.split()\n",
    "            ID1, ID2 = tmp[1], tmp[2]\n",
    "            key_cc = f\"#    {ID1}    {ID2}    0.0\\n\"\n",
    "            lines_cc = []\n",
    "            for line in ct_dict[key]:\n",
    "                tmp = line.split()\n",
    "                STA, TT1, TT2, WGT, PHA = (\n",
    "                    tmp[0],\n",
    "                    tmp[1],\n",
    "                    tmp[2],\n",
    "                    tmp[3],\n",
    "                    tmp[4],\n",
    "                )  ##HypoDD format\n",
    "\n",
    "                for i, row1 in picks.loc[(ID1, STA, PHA)].iterrows():\n",
    "\n",
    "                    data1 = collection.find_one(\n",
    "                        {\"_id\": f\"{row1['id']}_{row1['timestamp'].isoformat(timespec='milliseconds')}_{PHA}\"}\n",
    "                    )\n",
    "\n",
    "                    for j, row2 in picks.loc[(ID2, STA, PHA)].iterrows():\n",
    "\n",
    "                        data2 = collection.find_one(\n",
    "                            {\"_id\": f\"{row2['id']}_{row2['timestamp'].isoformat(timespec='milliseconds')}_{PHA}\"}\n",
    "                        )\n",
    "\n",
    "                        # if PHA == \"P\":  # Z\n",
    "                        #     waveform1 = np.array(data1[\"waveform\"])[lo:hi, -1:]\n",
    "                        #     waveform2 = np.array(data2[\"waveform\"])[lo:hi, -1:]\n",
    "                        # elif PHA == \"S\":  # E, N\n",
    "                        #     waveform1 = np.array(data1[\"waveform\"])[lo:hi, :-1]\n",
    "                        #     waveform2 = np.array(data2[\"waveform\"])[lo:hi, :-1]\n",
    "                        # else:\n",
    "                        #     raise (Exception(\"PHA must be P or S\"))\n",
    "                        waveform1 = np.array(data1[\"waveform\"])[lo:hi, :]\n",
    "                        waveform2 = np.array(data2[\"waveform\"])[lo:hi, :]\n",
    "\n",
    "                        cc = np.zeros(waveform1.shape[0])\n",
    "                        for k in range(waveform1.shape[1]):\n",
    "                            cc += np.correlate(waveform1[:, k], waveform2[:, k], mode=\"same\")\n",
    "                        norm = np.sqrt(np.sum(waveform1**2) * np.sum(waveform2**2))\n",
    "                        if norm == 0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            cc /= norm\n",
    "                        shift = (np.argmax(np.abs(cc)) - waveform1.shape[0] // 2) * dt + float(TT1) - float(TT2)\n",
    "                        coeff = np.max(np.abs(cc))\n",
    "\n",
    "                        if not np.isnan(coeff):\n",
    "                            lines_cc.append(f\"{STA:<7s}    {shift:.5f}    {coeff:.3f}    {PHA}\\n\")\n",
    "\n",
    "                cc_dict[key_cc] = lines_cc\n",
    "\n",
    "        return 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    processes = []\n",
    "    num_process = 16\n",
    "    for i in range(num_process):\n",
    "        p = Process(target=calc_cross_correlation, args=(keys[i::num_process], ct_dict, cc_dict))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    print(f\"{num_process} process: time = {time.time()-t0:.1f}\")\n",
    "\n",
    "    with open(cc_file, \"w\") as fcc:\n",
    "        for key in cc_dict:\n",
    "            fcc.write(key)\n",
    "            for line in cc_dict[key]:\n",
    "                fcc.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.397856Z",
     "iopub.status.busy": "2022-08-11T15:42:43.397747Z",
     "iopub.status.idle": "2022-08-11T15:42:43.415203Z",
     "shell.execute_reply": "2022-08-11T15:42:43.414898Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.397841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc_op = comp.func_to_container_op(\n",
    "    cross_correlation,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"tqdm\", \"minio\", \"pymongo\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.415715Z",
     "iopub.status.busy": "2022-08-11T15:42:43.415599Z",
     "iopub.status.idle": "2022-08-11T15:42:43.419952Z",
     "shell.execute_reply": "2022-08-11T15:42:43.419712Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.415700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hypodd_cc(\n",
    "    i: int,\n",
    "    config_json: InputPath(\"json\"),\n",
    "    ct_file: InputPath(str),\n",
    "    cc_file: InputPath(str),\n",
    "    event: InputPath(str),\n",
    "    station: InputPath(str),\n",
    "    inp_file: str = \"hypodd_cc.inp\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    from minio import Minio\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "\n",
    "    os.system(f\"cat {ct_file} > dt.ct\")\n",
    "    os.system(f\"cat {cc_file} > dt.cc\")\n",
    "    os.system(f\"cat {event} > event.sel\")\n",
    "    os.system(f\"cat {station} > stations_hypodd.dat \")\n",
    "\n",
    "    HYPODD_CMD = f\"HYPODD/src/hypodd/hypodd {inp_file}\"\n",
    "    if os.system(HYPODD_CMD) != 0:\n",
    "        raise (\"{HYPODD_CMD}\" + \" failed!\")\n",
    "    os.system(f\"mv hypodd.reloc hypodd_cc_{i:03d}.reloc\")\n",
    "\n",
    "    minioClient.fput_object(\n",
    "        bucket_name,\n",
    "        f\"{config['region']}/hypodd_cc_{i:03d}.reloc\",\n",
    "        f\"hypodd_cc_{i:03d}.reloc\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.420476Z",
     "iopub.status.busy": "2022-08-11T15:42:43.420355Z",
     "iopub.status.idle": "2022-08-11T15:42:43.430476Z",
     "shell.execute_reply": "2022-08-11T15:42:43.430247Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.420461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypodd_cc_op = comp.func_to_container_op(hypodd_cc, base_image=\"zhuwq0/hypodd-api:1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge hypodd results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.431076Z",
     "iopub.status.busy": "2022-08-11T15:42:43.430959Z",
     "iopub.status.idle": "2022-08-11T15:42:43.436084Z",
     "shell.execute_reply": "2022-08-11T15:42:43.435851Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.431061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_hypodd(\n",
    "    index: list,\n",
    "    config_json: InputPath(\"json\"),\n",
    "    catalog_ct: OutputPath(str),\n",
    "    catalog_cc: OutputPath(str),\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "    import json\n",
    "    import os\n",
    "    from glob import glob\n",
    "\n",
    "    from minio import Minio\n",
    "\n",
    "    minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    objects = minioClient.list_objects(bucket_name, prefix=f\"{config['region']}/hypodd/hypodd_\", recursive=True)\n",
    "\n",
    "    tmp_path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "    for obj in objects:\n",
    "        print(obj._object_name)\n",
    "        minioClient.fget_object(bucket_name, obj._object_name, tmp_path(obj._object_name.split(\"/\")[-1]))\n",
    "\n",
    "    # tmp_ct_catalogs = sorted(glob(tmp_path(\"hypodd_ct_*.reloc\")))\n",
    "    hypodd_ct_catalogs = [tmp_path(f\"hypodd_ct_{i:03d}.reloc\") for i in index]\n",
    "    print(f\"cat {' '.join(hypodd_ct_catalogs)} > {tmp_path('hypodd_ct_catalog.txt')}\")\n",
    "    os.system(f\"cat {' '.join(hypodd_ct_catalogs)} > {tmp_path('hypodd_ct_catalog.txt')}\")\n",
    "    minioClient.fput_object(\n",
    "        bucket_name,\n",
    "        f\"{config['region']}/hypodd_ct_catalog.txt\",\n",
    "        tmp_path(\"hypodd_ct_catalog.txt\"),\n",
    "    )\n",
    "    os.system(f\"cat {tmp_path('hypodd_ct_catalog.txt')} > {catalog_ct}\")\n",
    "\n",
    "    # hypodd_cc_catalogs = sorted(glob(tmp_path(\"hypodd_cc_*.reloc\")))\n",
    "    hypodd_cc_catalogs = [tmp_path(f\"hypodd_cc_{i:03d}.reloc\") for i in index]\n",
    "    print(f\"cat {' '.join(hypodd_cc_catalogs)} > {tmp_path('hypodd_cc_catalog.txt')}\")\n",
    "    os.system(f\"cat {' '.join(hypodd_cc_catalogs)} > {tmp_path('hypodd_cc_catalog.txt')}\")\n",
    "    minioClient.fput_object(\n",
    "        bucket_name,\n",
    "        f\"{config['region']}/hypodd_cc_catalog.txt\",\n",
    "        tmp_path(\"hypodd_cc_catalog.txt\"),\n",
    "    )\n",
    "    os.system(f\"cat {tmp_path('hypodd_cc_catalog.txt')} > {catalog_cc}\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.436589Z",
     "iopub.status.busy": "2022-08-11T15:42:43.436478Z",
     "iopub.status.idle": "2022-08-11T15:42:43.447253Z",
     "shell.execute_reply": "2022-08-11T15:42:43.446996Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.436574Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_hypodd_op = comp.func_to_container_op(\n",
    "    merge_hypodd,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"tqdm\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.447911Z",
     "iopub.status.busy": "2022-08-11T15:42:43.447801Z",
     "iopub.status.idle": "2022-08-11T15:42:43.457632Z",
     "shell.execute_reply": "2022-08-11T15:42:43.457433Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.447896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visulization(\n",
    "    config_json: InputPath(\"json\"),\n",
    "    hypodd_catalog_ct: InputPath(str),\n",
    "    hypodd_catalog_cc: InputPath(str),\n",
    "    gamma_catalog: InputPath(str),\n",
    "    standard_catalog: InputPath(str),\n",
    "    data_path: str = \"./\",\n",
    "    bucket_name: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = True,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    def plot3d(x, y, z, fig_name):\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter3d(\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    z=z,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=3.0, color=-z, cmin=-60, cmax=2, colorscale=\"Viridis\", opacity=0.8),\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis=dict(\n",
    "                    nticks=4,\n",
    "                    range=config[\"xlim_degree\"],\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    nticks=4,\n",
    "                    range=config[\"ylim_degree\"],\n",
    "                ),\n",
    "                zaxis=dict(\n",
    "                    nticks=4,\n",
    "                    # range=[z.max(), z.min()],\n",
    "                    range=[60, -2],\n",
    "                ),\n",
    "                #         aspectratio = dict(x=(xlim[1]-xlim[0])/2, y=(ylim[1]-ylim[0])/2, z=1),\n",
    "                aspectratio=dict(x=1, y=1, z=0.5),\n",
    "            ),\n",
    "            margin=dict(r=0, l=0, b=0, t=0),\n",
    "        )\n",
    "        fig.write_html(fig_name)\n",
    "\n",
    "    hypodd_ct_catalog = pd.read_csv(\n",
    "        hypodd_catalog_ct,\n",
    "        sep=\"\\s+\",\n",
    "        names=[\n",
    "            \"ID\",\n",
    "            \"LAT\",\n",
    "            \"LON\",\n",
    "            \"DEPTH\",\n",
    "            \"X\",\n",
    "            \"Y\",\n",
    "            \"Z\",\n",
    "            \"EX\",\n",
    "            \"EY\",\n",
    "            \"EZ\",\n",
    "            \"YR\",\n",
    "            \"MO\",\n",
    "            \"DY\",\n",
    "            \"HR\",\n",
    "            \"MI\",\n",
    "            \"SC\",\n",
    "            \"MAG\",\n",
    "            \"NCCP\",\n",
    "            \"NCCS\",\n",
    "            \"NCTP\",\n",
    "            \"NCTS\",\n",
    "            \"RCC\",\n",
    "            \"RCT\",\n",
    "            \"CID\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    hypodd_ct_catalog[\"MO\"][hypodd_ct_catalog[\"MO\"] == 0] = 1\n",
    "    hypodd_ct_catalog[\"DY\"][hypodd_ct_catalog[\"DY\"] == 0] = 1\n",
    "    hypodd_ct_catalog[\"time\"] = hypodd_ct_catalog.apply(\n",
    "        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{min(x[\"SC\"], 59.999):05.3f}',\n",
    "        axis=1,\n",
    "    )\n",
    "    hypodd_ct_catalog[\"magnitude\"] = hypodd_ct_catalog[\"MAG\"]\n",
    "    hypodd_ct_catalog[\"time\"] = hypodd_ct_catalog[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "    plot3d(\n",
    "        hypodd_ct_catalog[\"LON\"],\n",
    "        hypodd_ct_catalog[\"LAT\"],\n",
    "        hypodd_ct_catalog[\"DEPTH\"],\n",
    "        f\"{data_path}/hypodd_ct_catalog.html\",\n",
    "    )\n",
    "\n",
    "    gamma_catalog = pd.read_csv(gamma_catalog, parse_dates=[\"time\"])\n",
    "    gamma_catalog[\"depth_km\"] = gamma_catalog[\"depth(m)\"] / 1e3\n",
    "    plot3d(\n",
    "        gamma_catalog[\"longitude\"],\n",
    "        gamma_catalog[\"latitude\"],\n",
    "        gamma_catalog[\"depth_km\"],\n",
    "        f\"{data_path}/gamma_catalog.html\",\n",
    "    )\n",
    "\n",
    "    standard_catalog = pd.read_csv(standard_catalog, parse_dates=[\"time\"])\n",
    "    standard_catalog[\"depth_km\"] = standard_catalog[\"depth(m)\"] / 1e3\n",
    "    plot3d(\n",
    "        standard_catalog[\"longitude\"],\n",
    "        standard_catalog[\"latitude\"],\n",
    "        standard_catalog[\"depth_km\"],\n",
    "        f\"{data_path}/standard_catalog.html\",\n",
    "    )\n",
    "\n",
    "    ## histogram\n",
    "    bins = 30\n",
    "    config[\"starttime\"] = datetime.fromisoformat(config[\"starttime\"])\n",
    "    config[\"endtime\"] = datetime.fromisoformat(config[\"endtime\"])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(\n",
    "        gamma_catalog[\"time\"],\n",
    "        range=(config[\"starttime\"], config[\"endtime\"]),\n",
    "        bins=bins,\n",
    "        edgecolor=\"k\",\n",
    "        alpha=1.0,\n",
    "        linewidth=0.5,\n",
    "        label=f\"GaMMA: {len(gamma_catalog)}\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        hypodd_ct_catalog[\"time\"],\n",
    "        range=(config[\"starttime\"], config[\"endtime\"]),\n",
    "        bins=bins,\n",
    "        edgecolor=\"k\",\n",
    "        alpha=0.8,\n",
    "        linewidth=0.5,\n",
    "        label=f\"HypoDD: {len(hypodd_ct_catalog)}\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        standard_catalog[\"time\"],\n",
    "        range=(config[\"starttime\"], config[\"endtime\"]),\n",
    "        bins=bins,\n",
    "        edgecolor=\"k\",\n",
    "        alpha=0.6,\n",
    "        linewidth=0.5,\n",
    "        label=f\"Standard: {len(standard_catalog)}\",\n",
    "    )\n",
    "    # ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.autoscale(enable=True, axis=\"x\", tight=True)\n",
    "    # ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.autofmt_xdate()\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{data_path}/earthquake_frequency_time.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    # fig.savefig(f\"{data_path}/earthquake_number.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    xrange = (-1.0, max(standard_catalog[\"magnitude\"].max(), gamma_catalog[\"magnitude\"].max()))\n",
    "    ax.hist(\n",
    "        gamma_catalog[\"magnitude\"],\n",
    "        range=xrange,\n",
    "        bins=bins,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.5,\n",
    "        label=f\"GaMMA: {len(gamma_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        hypodd_ct_catalog[\"magnitude\"],\n",
    "        range=xrange,\n",
    "        bins=bins,\n",
    "        alpha=0.6,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.5,\n",
    "        label=f\"HypoDD: {len(hypodd_ct_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        standard_catalog[\"magnitude\"],\n",
    "        range=xrange,\n",
    "        bins=bins,\n",
    "        alpha=0.6,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.5,\n",
    "        label=f\"Standard: {len(standard_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.autoscale(enable=True, axis=\"x\", tight=True)\n",
    "    ax.set_xlabel(\"Magnitude\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    fig.savefig(f\"{data_path}/earthquake_magnitude_frequency.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    # fig.savefig(f\"{data_path}/earthquake_magnitude_frequency.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(\n",
    "        gamma_catalog[\"time\"],\n",
    "        gamma_catalog[\"magnitude\"],\n",
    "        \".\",\n",
    "        markersize=5.0,\n",
    "        alpha=1.0,\n",
    "        rasterized=True,\n",
    "        label=f\"GaMMA: {len(gamma_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        hypodd_ct_catalog[\"time\"],\n",
    "        hypodd_ct_catalog[\"magnitude\"],\n",
    "        \".\",\n",
    "        markersize=5.0,\n",
    "        alpha=1.0,\n",
    "        rasterized=True,\n",
    "        label=f\"HypoDD: {len(hypodd_ct_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        standard_catalog[\"time\"],\n",
    "        standard_catalog[\"magnitude\"],\n",
    "        \".\",\n",
    "        markersize=5.0,\n",
    "        alpha=1.0,\n",
    "        rasterized=True,\n",
    "        label=f\"Standard: {len(standard_catalog['magnitude'])}\",\n",
    "    )\n",
    "    ax.set_xlim(config[\"starttime\"], config[\"endtime\"])\n",
    "    ax.set_ylabel(\"Magnitude\")\n",
    "    # ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylim(bottom=-1)\n",
    "    ax.legend(markerscale=2)\n",
    "    ax.grid()\n",
    "    # ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.autofmt_xdate()\n",
    "    fig.savefig(f\"{data_path}/earthquake_magnitude_time.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    # fig.savefig(f\"{data_path}/earthquake_magnitude_time.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        hypodd_cc_catalog = pd.read_csv(\n",
    "            hypodd_catalog_cc,\n",
    "            sep=\"\\s+\",\n",
    "            names=[\n",
    "                \"ID\",\n",
    "                \"LAT\",\n",
    "                \"LON\",\n",
    "                \"DEPTH\",\n",
    "                \"X\",\n",
    "                \"Y\",\n",
    "                \"Z\",\n",
    "                \"EX\",\n",
    "                \"EY\",\n",
    "                \"EZ\",\n",
    "                \"YR\",\n",
    "                \"MO\",\n",
    "                \"DY\",\n",
    "                \"HR\",\n",
    "                \"MI\",\n",
    "                \"SC\",\n",
    "                \"MAG\",\n",
    "                \"NCCP\",\n",
    "                \"NCCS\",\n",
    "                \"NCTP\",\n",
    "                \"NCTS\",\n",
    "                \"RCC\",\n",
    "                \"RCT\",\n",
    "                \"CID\",\n",
    "            ],\n",
    "        )\n",
    "        hypodd_cc_catalog[\"time\"] = hypodd_cc_catalog.apply(\n",
    "            lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{min(x[\"SC\"], 59.999):05.3f}',\n",
    "            axis=1,\n",
    "        )\n",
    "        hypodd_cc_catalog[\"time\"] = hypodd_cc_catalog[\"time\"].apply(\n",
    "            lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        )\n",
    "        plot3d(\n",
    "            hypodd_cc_catalog[\"LON\"],\n",
    "            hypodd_cc_catalog[\"LAT\"],\n",
    "            hypodd_cc_catalog[\"DEPTH\"],\n",
    "            f\"{data_path}/hypodd_cc_catalog.html\",\n",
    "        )\n",
    "    except Exception as err:\n",
    "        print(f\"{err}\")\n",
    "\n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "\n",
    "        minioClient = Minio(s3_url, access_key=\"minio\", secret_key=\"minio123\", secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/hypodd_ct_catalog.html\",\n",
    "            f\"{data_path}/hypodd_ct_catalog.html\",\n",
    "        )\n",
    "\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/gamma_catalog.html\",\n",
    "            f\"{data_path}/gamma_catalog.html\",\n",
    "        )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/standard_catalog.html\",\n",
    "            f\"{data_path}/standard_catalog.html\",\n",
    "        )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/earthquake_frequency_time.png\",\n",
    "            f\"{data_path}/earthquake_frequency_time.png\",\n",
    "        )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/earthquake_magnitude_frequency.png\",\n",
    "            f\"{data_path}/earthquake_magnitude_frequency.png\",\n",
    "        )\n",
    "        minioClient.fput_object(\n",
    "            bucket_name,\n",
    "            f\"{config['region']}/earthquake_magnitude_time.png\",\n",
    "            f\"{data_path}/earthquake_magnitude_time.png\",\n",
    "        )\n",
    "        if os.path.exists(f\"{data_path}/hypodd_cc_catalog.html\"):\n",
    "            minioClient.fput_object(\n",
    "                bucket_name,\n",
    "                f\"{config['region']}/hypodd_cc_catalog.html\",\n",
    "                f\"{data_path}/hypodd_cc_catalog.html\",\n",
    "            )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.458125Z",
     "iopub.status.busy": "2022-08-11T15:42:43.458012Z",
     "iopub.status.idle": "2022-08-11T15:42:43.460480Z",
     "shell.execute_reply": "2022-08-11T15:42:43.460272Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.458110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    visulization(\n",
    "        root_dir(\"config.json\"),\n",
    "        root_dir(\"hypodd_ct_catalog.txt\"),\n",
    "        root_dir(\"hypodd_cc_catalog.txt\"),\n",
    "        root_dir(\"gamma_catalog.csv\"),\n",
    "        root_dir(\"standard_catalog.csv\"),\n",
    "        root_dir(\"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.461051Z",
     "iopub.status.busy": "2022-08-11T15:42:43.460862Z",
     "iopub.status.idle": "2022-08-11T15:42:43.480652Z",
     "shell.execute_reply": "2022-08-11T15:42:43.480381Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.461035Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visulization_op = comp.func_to_container_op(\n",
    "    visulization,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"matplotlib\", \"pandas\", \"plotly\", \"minio\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parallel process on cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.481317Z",
     "iopub.status.busy": "2022-08-11T15:42:43.481182Z",
     "iopub.status.idle": "2022-08-11T15:42:43.491801Z",
     "shell.execute_reply": "2022-08-11T15:42:43.491517Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.481301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"QuakeFlow\", description=\"\")\n",
    "def quakeflow_pipeline(\n",
    "    data_path: str = \"/tmp/\",\n",
    "    num_parallel=0,\n",
    "    bucket_catalog: str = \"catalogs\",\n",
    "    s3_url: str = \"minio-service:9000\",\n",
    "    secure: bool = False,\n",
    "):\n",
    "\n",
    "    config = (\n",
    "        config_op(region_name, num_parallel, bucket_name=f\"catalogs\", s3_url=s3_url, secure=secure)\n",
    "        .set_retry(3)\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "        .set_cpu_request(\"700m\")\n",
    "    )\n",
    "    config.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    events = (\n",
    "        download_events_op(config.outputs[\"config_json\"], bucket_name=f\"catalogs\", s3_url=s3_url, secure=secure)\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-16g\")\n",
    "        .set_retry(3)\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_request(\"700m\")\n",
    "        .set_display_name(\"Download Events\")\n",
    "    )\n",
    "    events.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    stations = (\n",
    "        download_stations_op(config.outputs[\"config_json\"], bucket_name=f\"catalogs\", s3_url=s3_url, secure=secure)\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "        .set_retry(3)\n",
    "        .set_cpu_request(\"700m\")\n",
    "        .set_display_name(\"Download Stations\")\n",
    "    )\n",
    "    stations.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    with kfp.dsl.ParallelFor(config.outputs[\"output\"]) as i:\n",
    "\n",
    "        vop_ = dsl.VolumeOp(\n",
    "            name=f\"Create volume {region_name}\",\n",
    "            resource_name=f\"data-volume-{str(i)}\",\n",
    "            size=\"120Gi\",\n",
    "            modes=dsl.VOLUME_MODE_RWO,\n",
    "        ).set_retry(3)\n",
    "\n",
    "        download_op_ = (\n",
    "            download_waveform_op(\n",
    "                i,\n",
    "                config.outputs[\"index_json\"],\n",
    "                config.outputs[\"config_json\"],\n",
    "                config.outputs[\"datetime_json\"],\n",
    "                stations.outputs[\"station_pkl\"],\n",
    "                data_path=data_path,\n",
    "                bucket_name=\"waveforms\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            )\n",
    "            .add_pvolumes({data_path: vop_.volume})\n",
    "            .set_cpu_request(\"700m\")\n",
    "            # .set_cpu_request(\"350m\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "            .set_retry(5)\n",
    "            .set_display_name(\"Download Waveforms\")\n",
    "        )\n",
    "        download_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "        phasenet_op_ = (\n",
    "            phasenet_op(\n",
    "                download_op_.outputs[\"Output\"],\n",
    "                download_op_.outputs[\"fname_csv\"],\n",
    "                stations.outputs[\"station_json\"],\n",
    "            )\n",
    "            .add_pvolumes({data_path: download_op_.pvolume})\n",
    "            .set_memory_request(\"9G\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"pool-16g\")\n",
    "            .set_retry(5)\n",
    "            .set_display_name(\"PhaseNet Picking\")\n",
    "        )\n",
    "        phasenet_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        phasenet_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "        gamma_op_ = (\n",
    "            gamma_op(\n",
    "                i,\n",
    "                config.outputs[\"index_json\"],\n",
    "                config.outputs[\"config_json\"],\n",
    "                phasenet_op_.outputs[\"picks\"],\n",
    "                stations.outputs[\"station_json\"],\n",
    "                bucket_name=f\"catalogs\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            )\n",
    "            .set_cpu_request(\"800m\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"pool-16g\")\n",
    "            .set_retry(3)\n",
    "            .set_display_name(\"GaMMA Association\")\n",
    "        )\n",
    "        gamma_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "        # resume_op_ = (\n",
    "        #     resume_waveform_op(\n",
    "        #         i,\n",
    "        #         config.outputs[\"index_json\"],\n",
    "        #         config.outputs[\"config_json\"],\n",
    "        #         config.outputs[\"datetime_json\"],\n",
    "        #         data_path=data_path,\n",
    "        #         bucket_name=\"waveforms\",\n",
    "        #         s3_url=s3_url,\n",
    "        #         secure=secure,\n",
    "        #     )\n",
    "        #     .add_pvolumes({data_path: vop_.volume})\n",
    "        #     .set_cpu_request(\"700m\")\n",
    "        #     # .set_cpu_request(\"350m\")\n",
    "        #     .set_retry(5)\n",
    "        #     .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "        #     .set_display_name(\"Resume Waveforms\")\n",
    "        # )\n",
    "        # resume_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "        # save_op_ = (\n",
    "        #     save_waveform_op(\n",
    "        #         i,\n",
    "        #         config.outputs[\"index_json\"],\n",
    "        #         config.outputs[\"config_json\"],\n",
    "        #         config.outputs[\"datetime_json\"],\n",
    "        #         data_path=data_path,\n",
    "        #         bucket_name=\"waveforms\",\n",
    "        #         s3_url=s3_url,\n",
    "        #         secure=secure,\n",
    "        #     )\n",
    "        #     .add_pvolumes({data_path: vop_.volume})\n",
    "        #     .set_cpu_request(\"700m\")\n",
    "        #     # .set_cpu_request(\"350m\")\n",
    "        #     .set_retry(5)\n",
    "        #     .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "        #     .set_display_name(\"Save Waveforms\")\n",
    "        # )\n",
    "        # save_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    merge_op_ = (\n",
    "        merge_op(\n",
    "            config.outputs[\"index_json\"],\n",
    "            config.outputs[\"config_json\"],\n",
    "            bucket_name=f\"catalogs\",\n",
    "            s3_url=s3_url,\n",
    "            secure=secure,\n",
    "        )\n",
    "        .after(gamma_op_)\n",
    "        .set_memory_request(\"12G\")\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-64g\")\n",
    "        .set_display_name(\"Merge Catalog\")\n",
    "    )\n",
    "    merge_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    convert_station_op_ = (\n",
    "        convert_station_op(\n",
    "            config_json=config.outputs[\"config_json\"],\n",
    "            station_json=stations.outputs[\"station_json\"],\n",
    "            bucket_name=f\"catalogs\",\n",
    "            s3_url=s3_url,\n",
    "            secure=secure,\n",
    "        )\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-2g\")\n",
    "        .set_display_name(\"Convert Station Format\")\n",
    "    )\n",
    "    split_hypodd_op_ = (\n",
    "        split_hypodd_op(\n",
    "            config.outputs[\"config_json\"],\n",
    "            catalog_csv=merge_op_.outputs[\"catalog_csv\"],\n",
    "        )\n",
    "        .after(merge_op_)\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-64g\")\n",
    "        .set_display_name(\"Split Catalog\")\n",
    "    )\n",
    "    split_hypodd_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "    split_hypodd_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "    with kfp.dsl.ParallelFor(split_hypodd_op_.outputs[\"output\"]) as i:\n",
    "\n",
    "        convert_phase_op_ = (\n",
    "            convert_phase_op(\n",
    "                i,\n",
    "                config_json=config.outputs[\"config_json\"],\n",
    "                picks_csv=merge_op_.outputs[\"picks_csv\"],\n",
    "                catalog_csv=merge_op_.outputs[\"catalog_csv\"],\n",
    "                bucket_name=\"catalogs\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            )\n",
    "            .set_retry(3)\n",
    "            .set_display_name(\"Convert Phase Format\")\n",
    "            # .set_memory_request(\"48G\")\n",
    "            # .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-64g\")\n",
    "            .set_memory_request(\"12G\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"pool-16g\")\n",
    "        )\n",
    "        convert_phase_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        convert_phase_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "        ph2dt_op_ = (\n",
    "            ph2dt_op(\n",
    "                i,\n",
    "                config_json=config.outputs[\"config_json\"],\n",
    "                hypodd_phase=convert_phase_op_.outputs[\"hypodd_phase\"],\n",
    "                station_dat=convert_station_op_.outputs[\"hypodd_station\"],\n",
    "                bucket_name=\"catalogs\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            )\n",
    "            .set_display_name(\"PH2DT\")\n",
    "            .set_memory_request(\"12G\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-16g\")\n",
    "        )\n",
    "        ph2dt_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        ph2dt_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "        hypodd_ct_op_ = (\n",
    "            hypodd_ct_op(\n",
    "                i,\n",
    "                config_json=config.outputs[\"config_json\"],\n",
    "                ct=ph2dt_op_.outputs[\"ct\"],\n",
    "                event=ph2dt_op_.outputs[\"hypodd_event\"],\n",
    "                station=convert_station_op_.outputs[\"hypodd_station\"],\n",
    "                bucket_name=\"catalogs\",\n",
    "                s3_url=s3_url,\n",
    "                secure=secure,\n",
    "            )\n",
    "            .set_display_name(\"HypoDD\")\n",
    "            .set_memory_request(\"12G\")\n",
    "            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"pool-16g\")\n",
    "        )\n",
    "        hypodd_ct_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        hypodd_ct_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "        # cc_op_ = cc_op(\n",
    "        #     ct=ph2dt_op_.outputs[\"ct\"],\n",
    "        #     picks=merge_op_.outputs[\"picks_csv\"],\n",
    "        #     catalog=merge_op_.outputs[\"catalog_csv\"],\n",
    "        # ).set_display_name('Cross Correlation')\n",
    "        # cc_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        # cc_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "        # hypodd_cc_op_ = hypodd_cc_op(\n",
    "        #     i,\n",
    "        #     config_json=config.outputs[\"config_json\"],\n",
    "        #     ct=ph2dt_op_.outputs[\"ct\"],\n",
    "        #     cc=cc_op_.outputs[\"cc\"],\n",
    "        #     event=ph2dt_op_.outputs[\"hypodd_event\"],\n",
    "        #     station=convert_station_op_.outputs[\"hypodd_station\"],\n",
    "        #     bucket_name=\"catalogs\",\n",
    "        #     s3_url=s3_url,\n",
    "        #     secure=secure,\n",
    "        # ).set_display_name('HypoDD + CC')\n",
    "        # hypodd_cc_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        # hypodd_cc_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "    merge_hypodd_op_ = (\n",
    "        merge_hypodd_op(\n",
    "            split_hypodd_op_.outputs[\"output\"],\n",
    "            config_json=config.outputs[\"config_json\"],\n",
    "            bucket_name=f\"catalogs\",\n",
    "            s3_url=s3_url,\n",
    "            secure=secure,\n",
    "        )\n",
    "        .after(hypodd_ct_op_)\n",
    "        # .after(hypodd_cc_op_)\n",
    "        .set_display_name(\"Merge Catalog\")\n",
    "        .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"spot-16g\")\n",
    "    )\n",
    "    merge_hypodd_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    merge_hypodd_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "    visulization_op_ = (\n",
    "        visulization_op(\n",
    "            config_json=config.outputs[\"config_json\"],\n",
    "            hypodd_catalog_ct=merge_hypodd_op_.outputs[\"catalog_ct\"],\n",
    "            hypodd_catalog_cc=merge_hypodd_op_.outputs[\"catalog_cc\"],\n",
    "            gamma_catalog=merge_op_.outputs[\"catalog_csv\"],\n",
    "            standard_catalog=events.outputs[\"standard_catalog\"],\n",
    "            bucket_name=f\"catalogs\",\n",
    "            s3_url=s3_url,\n",
    "            secure=secure,\n",
    "        )\n",
    "        .after(merge_hypodd_op_)\n",
    "        .set_display_name(\"Visulization\")\n",
    "    )\n",
    "    visulization_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    visulization_op_.set_image_pull_policy(\"Always\")\n",
    "\n",
    "    #### vop_.delete().after(merge_hypodd_op_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-11T15:42:43.492378Z",
     "iopub.status.busy": "2022-08-11T15:42:43.492260Z",
     "iopub.status.idle": "2022-08-11T15:42:45.316808Z",
     "shell.execute_reply": "2022-08-11T15:42:45.316527Z",
     "shell.execute_reply.started": "2022-08-11T15:42:43.492362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://56496b41855e5ce2-dot-us-west1.pipelines.googleusercontent.com/#/experiments/details/dffd451b-47f0-48ae-9895-9d7cf0fc372b\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://56496b41855e5ce2-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/afa3baaa-75ae-49aa-b3d4-14e9f82a6719\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow_zhuwq.json\"\n",
    "experiment_name = \"QuakeFlow\"\n",
    "pipeline_func = quakeflow_pipeline\n",
    "run_name = f\"{pipeline_func.__name__}_{region_name.lower()}\"\n",
    "\n",
    "arguments = {\n",
    "    \"data_path\": \"/tmp\",\n",
    "    \"num_parallel\": 0,\n",
    "    \"bucket_catalog\": \"catalogs\",\n",
    "    \"s3_url\": \"minio-service:9000\",\n",
    "    # \"s3_url\": \"quakeflow-minio:9000\",\n",
    "    \"secure\": False,\n",
    "}\n",
    "\n",
    "if not run_local:\n",
    "    pipeline_conf = kfp.dsl.PipelineConf()\n",
    "    pipeline_conf.set_image_pull_policy(\"Always\")\n",
    "    pipeline_conf.ttl_seconds_after_finished = 60 * 10\n",
    "    client = kfp.Client(host=\"56496b41855e5ce2-dot-us-west1.pipelines.googleusercontent.com\")\n",
    "    # client = kfp.Client(host=\"http://localhost:8080\")\n",
    "    kfp.compiler.Compiler().compile(pipeline_func, \"{}.zip\".format(experiment_name), pipeline_conf=pipeline_conf)\n",
    "    results = client.create_run_from_pipeline_func(\n",
    "        pipeline_func,\n",
    "        experiment_name=experiment_name,\n",
    "        run_name=run_name,\n",
    "        arguments=arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0efb5d07c150d814a79610ed835fac9f37a29f75f64726a0e33cb3dca03bca5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
